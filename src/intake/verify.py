#!/usr/bin/env -S uvx --with-requirements requirements.txt python
"""
Inspect the contents of R2R document store.

Requirements:
    - Python 3.11+
    - r2r Python SDK and Pandas (the shebang above will create a temp venv)
"""

import argparse
import json
import logging
import re
import sys

import pandas as pd
from config import R2R_DEFAULT_BASE_URL, setup_logging
from r2r import R2RClient

DOCUMENTS_FILE = "documents.csv"
DOCUMENTS_COLUMNS = [
    "id", "title", "size_in_bytes", "ingestion_status", "extraction_status",
    "created_at", "updated_at", "type", "metadata"
]
COLUMNS_TYPES = {
    "id": "string",
    "title": "string",
    "size_in_bytes": "int64",
    "ingestion_status": "string",
    "extraction_status": "string",
    "type": "string",
    "metadata": "string"
}
DATE_COLUMNS = ["created_at", "updated_at"]

def check_r2r(client: R2RClient) -> bool:
    """Check that R2R is up and replying."""
    try:
        _ = client.documents.list(limit=1)
        logging.debug("R2R client is up and responding.")
        return True
    except Exception as e:
        logging.warning(f"Failed to get reply from R2R: {e}")
        return False

def get_existing_documents(client: R2RClient) -> pd.DataFrame | None:
    """
    Retrieve all documents from the R2R service as a structured DataFrame.

    This function:
    - Exports document metadata from the R2R client to a CSV file.
    - Loads the CSV file into a pandas DataFrame with proper type and date parsing.
    - Parses the 'metadata' column as JSON and flattens it into prefixed columns.

    Args:
        client (R2RClient): The connected R2R client instance.

    Returns:
        pd.DataFrame | None: A DataFrame with one row per document and enriched metadata columns,
        or None if retrieval or parsing fails.

    """
    try:
        logging.debug(f"Exporting documents known to R2R into {DOCUMENTS_FILE}...")
        client.documents.export(output_path=DOCUMENTS_FILE, columns=DOCUMENTS_COLUMNS)
        response = pd.read_csv(DOCUMENTS_FILE, dtype=COLUMNS_TYPES, parse_dates=DATE_COLUMNS)
        response["metadata"] = response["metadata"].apply(json.loads)
        metadata_df = pd.json_normalize(response["metadata"]).add_prefix("meta_")
        response = response.drop(columns=["metadata"]).join(metadata_df)
        logging.info(f"Successfully obtained {len(response)} document records from R2R")
        return response
    except Exception as e:
        logging.error(f"Error fetching document list: {e}")
        return None

def describe_table(documents: pd.DataFrame) -> None:
    """
    Print basic description of the R2R document store contents.

    Returns:
        None

    """
    documents.info()
    print("\nüìä Summary statistics (numerical columns):")
    num_stats = documents.describe(include=[float, int]).round(0).astype("Int64")
    formatted_stats = num_stats.apply(lambda col: col.map("{:,}".format))
    print(formatted_stats)

    print("\nüìù Summary statistics (object/string columns):")
    obj_stats = documents.describe(include=[object, "string"]).transpose()
    print(obj_stats)

def show_short_titles(documents: pd.DataFrame) -> int:
    """
    Print and count documents with null or one word title.

    Returns:
        int: Total number of documents with suspiciously short title.

    """
    if "title" not in documents.columns:
        print("‚ö†Ô∏è No 'title' column found.")
        return 1
    if "meta_title" not in documents.columns:
        print("‚ö†Ô∏è No 'meta_title' column found.")
        return 2

    def is_anomalous(title):
        if pd.isna(title):
            return True
        title = str(title).strip()
        return title == "" or len(title.split()) <= 1

    anomalies = documents[documents["title"].apply(is_anomalous)]
    anomalies = anomalies.sort_values("title", na_position="first")
    print(f"\nüö® Found {len(anomalies)} anomalous title(s):")
    for _, row in anomalies.iterrows():
        print(f"‚Ä¢ ID: {row['id']} ‚Äî Title: '{row['title']}' ‚Äî Meta_title: '{row['meta_title']}' ‚Äî Meta_source_url: '{row['meta_source_url']}'")
    return len(anomalies)

def show_failed_ingestions(documents: pd.DataFrame) -> int:
    """
    Print and count documents with unsuccessfull ingestion status.

    Returns:
        int: Total number of documents with unsuccessfull ingestion status.

    """
    if "ingestion_status" not in documents.columns:
        print("‚ö†Ô∏è No 'ingestion_status' column found.")
        return 0
    failures = documents[documents["ingestion_status"] != "success"]
    if failures.empty:
        print("‚úÖ All documents were successfully ingested.")
        return 0
    print(f"\n‚ùå Found {len(failures)} documents with failed ingestion:")
    for _, row in failures.iterrows():
        print(f"‚Ä¢ ID: {row['id']}\n  Status: {row['ingestion_status']}\n  Failure: {row.get('meta_failure', 'N/A')}")
    return len(failures)

def show_repeat_halid(documents: pd.DataFrame) -> int:
    """
    Print and count sets of documents with repeated HAL Id.

    Returns:
        int: Total number of documents with repeated HAL Ids.

    """
    if "meta_hal_id" not in documents.columns:
        print("‚ö†Ô∏è No 'meta_hal_id' column found.")
        return 0
    dup_mask = documents["meta_hal_id"].duplicated(keep=False)
    duplicates = documents[dup_mask & documents["meta_hal_id"].notna()].sort_values("meta_hal_id")
    if duplicates.empty:
        print("‚úÖ No repeated HAL IDs found.")
        return 0
    print(f"\nüîÅ Found {len(duplicates)} documents with repeated HAL IDs:")
    grouped = duplicates.groupby("meta_hal_id")
    for hal_id, group in grouped:
        print(f"\n‚Ä¢ HAL ID: {hal_id}")
        for _, row in group.iterrows():
            print(f"  - ID: {row['id']} - Ingestion: {row['ingestion_status']}, Extraction: {row['extraction_status']}")
    return len(duplicates)

def normalize_title(title: str) -> str:
    """Normalize title by lowercasing, stripping punctuation, and collapsing spaces."""
    if pd.isna(title):
        return ""
    title = str(title).lower()
    title = re.sub(r'[^\w\s]', '', title)
    title = re.sub(r'\s+', ' ', title).strip()
    return title

def show_repeat_titles(documents: pd.DataFrame) -> int:
    """
    Print and count sets of documents with repeated titles.

    Ignores casing, spacing and punctuation when comparing titles.

    Returns:
        int: Total number of documents with non-unique normalized titles.

    """
    if "title" not in documents.columns:
        print("‚ö†Ô∏è No 'title' column found.")
        return 0
    documents["normalized_title"] = documents["title"].apply(normalize_title)
    dup_mask = documents.duplicated("normalized_title", keep=False)
    duplicates = documents[dup_mask].sort_values("normalized_title")
    if duplicates.empty:
        print("‚úÖ No repeated titles found.")
        return 0
    print(f"\nüîÅ Found {len(duplicates)} documents with repeated titles (normalized):")
    grouped = duplicates.groupby("normalized_title")
    for norm_title, group in grouped:
        print(f"\n‚Ä¢ Normalized title: '{norm_title}'")
        for _, row in group.iterrows():
            print(
                f"  - ID: {row['id']}\n"
                f"    Title: '{row['title']}'\n"
                f"    Size: {row['size_in_bytes']:,} bytes\n"
                f"    Ingestion: {row['ingestion_status']}, Extraction: {row['extraction_status']}\n"
                f"    HAL ID: {row.get('meta_hal_id', 'N/A')}\n"
                f"    Citation: {row.get('meta_citation', 'N/A')}"
            )
    return len(duplicates)

def setup() -> tuple[R2RClient, object]:
    """Retrieve the documents in store."""
    setup_logging()
    logging.info(f"Connecting to R2R at {R2R_DEFAULT_BASE_URL}...")
    client = R2RClient(base_url=R2R_DEFAULT_BASE_URL)

    if not check_r2r(client):
        logging.error("R2R service is unreachable.")
        exit(3)

    documents = get_existing_documents(client)
    if documents is None:
        logging.error("Could not retrieve documents from R2R.")
        exit(1)

    return client, documents


COMMANDS = {
    "overview": describe_table,
    "short-titles": show_short_titles,
    "failed-ingestions": show_failed_ingestions,
    "repeat-halid": show_repeat_halid,
    "repeat-titles": show_repeat_titles,
}

def get_help_lines() -> str:
    """Fetch the help from the functions docstrings."""
    max_len = max(len(cmd) for cmd in COMMANDS)
    lines = [
        f"  {cmd.ljust(max_len)}  {fn.__doc__.strip().splitlines()[0]}"
        for cmd, fn in COMMANDS.items()
    ]
    lines.append(f"  {'all'.ljust(max_len)}  Run all checks in sequence.")
    return "\n".join(lines)

def main():
    """Inspect the state of the R2R documents store."""
    parser = argparse.ArgumentParser(
        description=f"Inspect the state of the R2R documents store at {R2R_DEFAULT_BASE_URL}.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "what",
        help="What to inspect, must be one of the following:\n" + get_help_lines()
    )

    if len(sys.argv) == 1:
        parser.print_help(sys.stderr)
        sys.exit(2)

    args = parser.parse_args()

    if args.what not in COMMANDS and args.what != "all":
        parser.error(f"Unknown command: {args.what}")

    _, documents = setup()

    if args.command == "all":
        for fn in COMMANDS.values():
            fn(documents)
    else:
        COMMANDS[args.command](documents)


if __name__ == "__main__":
    main()

