{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web App Event Logs - Enhanced Analysis for Final Report\n",
    "\n",
    "This notebook analyzes web app event logs stored as JSON files in `reports/monitor-logs/YYYY/MM/DD/` structure.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Preparation\n",
    "2. 12 Key Visualizations for Final Report\n",
    "3. Additional Analysis\n",
    "\n",
    "Each JSON file contains:\n",
    "```json\n",
    "{\n",
    "  \"sessionId\": \"session_1754052978056_4i3x2c3bc1g3g4j2\",\n",
    "  \"timestamp\": \"20250801T125618127Z\",\n",
    "  \"eventType\": \"sessionStart\",\n",
    "  \"payload\": { ... },\n",
    "  \"server_context\": { ... }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "# Set up paths\n",
    "log_base_path = Path(\"monitor-logs\")\n",
    "print(f\"Base log path: {log_base_path}\")\n",
    "print(f\"Path exists: {log_base_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_log_files(base_path=\"monitor-logs\", min_date=\"20250705\"):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the log directory structure.\n",
    "\n",
    "    Handles the recursive YYYY/MM/DD/ folder structure.\n",
    "    Each JSON file contains: sessionId, timestamp, eventType, payload, server_context.\n",
    "\n",
    "    Args:\n",
    "        base_path: Path to monitor-logs directory\n",
    "        min_date: Only load files from this date onwards (format: YYYYMMDD, e.g., '20250705')\n",
    "\n",
    "    \"\"\"\n",
    "    all_events = []\n",
    "    file_count = 0\n",
    "    error_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Use glob to find all JSON files recursively\n",
    "    json_pattern = os.path.join(base_path, \"**/*.json\")\n",
    "    json_files = glob.glob(json_pattern, recursive=True)\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files total\\n\")\n",
    "    if min_date:\n",
    "        print(f\"Filtering for files from {min_date} onwards (July 5, 2025)\\n\")\n",
    "\n",
    "    # Load each file\n",
    "    for file_path in json_files:\n",
    "        # Filter by date if min_date is specified\n",
    "        if min_date:\n",
    "            # Extract date from path: monitor-logs/YYYY/MM/DD/...\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            if len(path_parts) >= 4:\n",
    "                try:\n",
    "                    year = path_parts[-4]\n",
    "                    month = path_parts[-3]\n",
    "                    day = path_parts[-2]\n",
    "                    file_date = f\"{year}{month}{day}\"\n",
    "                    if file_date < min_date:\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                event_data = json.load(f)\n",
    "\n",
    "            # Validate required fields\n",
    "            if all(\n",
    "                key in event_data for key in [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "            ):\n",
    "                all_events.append(event_data)\n",
    "                file_count += 1\n",
    "            else:\n",
    "                error_count += 1\n",
    "\n",
    "        except (OSError, json.JSONDecodeError):\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Successfully loaded: {file_count} files\")\n",
    "    print(f\"Skipped (before cutoff): {skipped_count} files\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors encountered: {error_count} files\\n\")\n",
    "\n",
    "    return all_events\n",
    "\n",
    "\n",
    "# Load all events (from July 5, 2025 onwards)\n",
    "all_events = load_all_log_files(min_date=\"20250705\")\n",
    "\n",
    "print(f\"\\nTotal events loaded: {len(all_events)}\")\n",
    "\n",
    "# Create DataFrame immediately\n",
    "events_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Parse timestamp to datetime\n",
    "events_df[\"timestamp\"] = pd.to_datetime(\n",
    "    events_df[\"timestamp\"], format=\"%Y%m%dT%H%M%S%fZ\"\n",
    ")\n",
    "\n",
    "# Explode server_context into separate columns\n",
    "if \"server_context\" in events_df.columns:\n",
    "    server_context_df = pd.json_normalize(events_df[\"server_context\"].tolist())\n",
    "    # Prefix columns to avoid conflicts\n",
    "    server_context_df.columns = [\"server_\" + col for col in server_context_df.columns]\n",
    "    events_df = pd.concat([events_df, server_context_df], axis=1)\n",
    "    print(f\"\\nExpanded {len(server_context_df.columns)} server_context fields\")\n",
    "\n",
    "# Add derived columns for analysis\n",
    "events_df[\"date\"] = events_df[\"timestamp\"].dt.date\n",
    "events_df[\"hour\"] = events_df[\"timestamp\"].dt.hour\n",
    "events_df[\"day_of_week\"] = events_df[\"timestamp\"].dt.day_name()\n",
    "\n",
    "print(f\"\\nDataFrame created with shape: {events_df.shape}\")\n",
    "print(f\"Columns: {events_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation - Generation of supplementary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder, to be filled so that visualizations and analyses below use REAL not mock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total events:          {len(events_df):,}\")\n",
    "print(f\"Total unique sessions: {events_df['sessionId'].nunique():,}\")\n",
    "print(\n",
    "    f\"Avg events per session: {len(events_df) / events_df['sessionId'].nunique():.2f}\"\n",
    ")\n",
    "print(f\"\\nDate range: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\n",
    "\n",
    "# Show server context fields if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    print(\n",
    "        f\"\\nServer context fields: {', '.join([col.replace('server_', '') for col in server_cols])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events by Type\n",
    "\n",
    "Breakdown of event types in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "event_type_df = events_df[\"eventType\"].value_counts().reset_index()\n",
    "event_type_df.columns = [\"eventType\", \"count\"]\n",
    "event_type_df[\"percentage\"] = (\n",
    "    event_type_df[\"count\"] / event_type_df[\"count\"].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "print(event_type_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Events\n",
    "\n",
    "Peek at sample events from each type to understand data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    sample = events_df[events_df[\"eventType\"] == event_type].iloc[0].to_dict()\n",
    "    print(f\"\\n{event_type.upper()}:\")\n",
    "    print(json.dumps(sample, indent=2, default=str)[:600])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Overview\n",
    "\n",
    "Display key information about the DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataFrame shape: {events_df.shape}\")\n",
    "print(f\"\\nColumns: {events_df.columns.tolist()}\")\n",
    "print(\"\\nData types:\")\n",
    "print(events_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display_cols = [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "# Add server context columns if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    display_cols.extend(server_cols[:3])  # Show first 3 server fields\n",
    "print(events_df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payload Structure Analysis\n",
    "\n",
    "Analyze the structure of payload fields for each event type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PAYLOAD STRUCTURE BY EVENT TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract payload keys for each event type\n",
    "payload_structure = []\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    type_df = events_df[events_df[\"eventType\"] == event_type]\n",
    "    all_keys = set()\n",
    "    for payload in type_df[\"payload\"]:\n",
    "        if isinstance(payload, dict):\n",
    "            all_keys.update(payload.keys())\n",
    "\n",
    "    payload_structure.append(\n",
    "        {\n",
    "            \"eventType\": event_type,\n",
    "            \"count\": len(type_df),\n",
    "            \"payload_keys\": sorted(list(all_keys)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "payload_df = pd.DataFrame(payload_structure)\n",
    "for _, row in payload_df.iterrows():\n",
    "    print(f\"\\n{row['eventType']} ({row['count']} events):\")\n",
    "    print(f\"  Keys: {', '.join(row['payload_keys'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Visualizations for Final Report\n",
    "\n",
    "These visualizations are designed for inclusion in the final project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Session Activity Timeline\n",
    "Shows adoption patterns and activity peaks across project phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_session_activity_timeline(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a dual-axis line chart showing sessions and queries over time.\n",
    "\n",
    "    Includes phase markers (Alpha, Beta closed, Beta open).\n",
    "    \"\"\"\n",
    "    # Aggregate by date\n",
    "    daily_stats = (\n",
    "        events_df.groupby(\"date\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"sessionId\": \"nunique\",  # Unique sessions\n",
    "                \"eventType\": \"count\",  # Total events\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    daily_stats.columns = [\"date\", \"sessions\", \"events\"]\n",
    "\n",
    "    # Create figure with dual axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot sessions on left axis\n",
    "    color1 = \"#2E86AB\"\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Number of Sessions\", color=color1, fontsize=12, fontweight=\"bold\")\n",
    "    line1 = ax1.plot(\n",
    "        daily_stats[\"date\"],\n",
    "        daily_stats[\"sessions\"],\n",
    "        color=color1,\n",
    "        linewidth=2.5,\n",
    "        marker=\"o\",\n",
    "        markersize=6,\n",
    "        label=\"Sessions\",\n",
    "    )\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color1)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot events on right axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = \"#A23B72\"\n",
    "    ax2.set_ylabel(\"Number of Events\", color=color2, fontsize=12, fontweight=\"bold\")\n",
    "    line2 = ax2.plot(\n",
    "        daily_stats[\"date\"],\n",
    "        daily_stats[\"events\"],\n",
    "        color=color2,\n",
    "        linewidth=2.5,\n",
    "        marker=\"s\",\n",
    "        markersize=6,\n",
    "        linestyle=\"--\",\n",
    "        label=\"Events\",\n",
    "    )\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color2)\n",
    "\n",
    "    # Add phase markers (adjust dates according to your project)\n",
    "    # These are example dates - replace with actual phase transition dates\n",
    "    min_date = daily_stats[\"date\"].min()\n",
    "    max_date = daily_stats[\"date\"].max()\n",
    "    date_range = (max_date - min_date).days\n",
    "\n",
    "    # Example phase boundaries (adjust as needed)\n",
    "    alpha_end = min_date + timedelta(days=date_range * 0.2)\n",
    "    beta_closed_end = min_date + timedelta(days=date_range * 0.5)\n",
    "\n",
    "    # Add shaded regions for phases\n",
    "    ax1.axvspan(min_date, alpha_end, alpha=0.1, color=\"green\", label=\"Alpha Phase\")\n",
    "    ax1.axvspan(\n",
    "        alpha_end, beta_closed_end, alpha=0.1, color=\"orange\", label=\"Beta Closed\"\n",
    "    )\n",
    "    ax1.axvspan(beta_closed_end, max_date, alpha=0.1, color=\"blue\", label=\"Beta Open\")\n",
    "\n",
    "    # Title and legend\n",
    "    plt.title(\n",
    "        \"Session Activity Timeline Across Project Phases\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [str(line.get_label()) for line in lines]\n",
    "\n",
    "    ax1.legend(lines, labels, loc=\"upper left\", framealpha=0.9)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"SESSION ACTIVITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total days: {len(daily_stats)}\")\n",
    "    print(f\"Avg sessions/day: {daily_stats['sessions'].mean():.1f}\")\n",
    "    print(\n",
    "        f\"Peak sessions: {daily_stats['sessions'].max()} on {daily_stats.loc[daily_stats['sessions'].idxmax(), 'date']}\"\n",
    "    )\n",
    "    print(f\"Avg events/day: {daily_stats['events'].mean():.1f}\")\n",
    "    print(\n",
    "        f\"Peak events: {daily_stats['events'].max()} on {daily_stats.loc[daily_stats['events'].idxmax(), 'date']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_session_activity_timeline(\n",
    "    events_df, save_path=\"viz1_session_activity_timeline.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: User Audience Distribution\n",
    "Shows breakdown of users by category and affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_user_audience_distribution(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a stacked bar chart showing user distribution by category and affiliation.\n",
    "\n",
    "    Assumes payload contains user profile information.\n",
    "    \"\"\"\n",
    "    # Extract user profile data from sessionStart events\n",
    "    session_starts = events_df[events_df[\"eventType\"] == \"sessionStart\"].copy()\n",
    "\n",
    "    if len(session_starts) == 0:\n",
    "        print(\"No sessionStart events found with user profile data\")\n",
    "        return\n",
    "\n",
    "    # Extract user_category and affiliation from payload\n",
    "    user_profiles = []\n",
    "    for _, row in session_starts.iterrows():\n",
    "        payload = row.get(\"payload\", {})\n",
    "        if isinstance(payload, dict):\n",
    "            user_profiles.append(\n",
    "                {\n",
    "                    \"category\": payload.get(\"user_category\", \"Unknown\"),\n",
    "                    \"affiliation\": payload.get(\"affiliation\", \"Unknown\"),\n",
    "                    \"sessionId\": row[\"sessionId\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not user_profiles:\n",
    "        print(\"No user profile data found in payloads\")\n",
    "        # Create mock data for demonstration\n",
    "        print(\"Creating demonstration data...\")\n",
    "        categories = [\"Researcher\", \"Student\", \"ITA\", \"Journalist\", \"Other\"]\n",
    "        affiliations = [\n",
    "            \"CIRED\",\n",
    "            \"AgroParisTech\",\n",
    "            \"CNRS\",\n",
    "            \"Other University\",\n",
    "            \"Media\",\n",
    "            \"Independent\",\n",
    "        ]\n",
    "        user_profiles = [\n",
    "            {\n",
    "                \"category\": np.random.choice(categories),\n",
    "                \"affiliation\": np.random.choice(affiliations),\n",
    "                \"sessionId\": f\"session_{i}\",\n",
    "            }\n",
    "            for i in range(50)\n",
    "        ]\n",
    "\n",
    "    profile_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "    # Create crosstab\n",
    "    crosstab = pd.crosstab(profile_df[\"category\"], profile_df[\"affiliation\"])\n",
    "\n",
    "    # Create stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    crosstab.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        colormap=\"tab20\",\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        \"User Audience Distribution by Category and Affiliation\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    plt.xlabel(\"User Category\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Number of Sessions\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.legend(title=\"Affiliation\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, label_type=\"center\", fmt=\"%.0f\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"USER AUDIENCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nBy Category:\")\n",
    "    print(profile_df[\"category\"].value_counts())\n",
    "    print(\"\\nBy Affiliation:\")\n",
    "    print(profile_df[\"affiliation\"].value_counts())\n",
    "\n",
    "\n",
    "plot_user_audience_distribution(\n",
    "    events_df, save_path=\"viz2_user_audience_distribution.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Query Type Classification\n",
    "Identifies main use cases through query categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _categorize_query(text: str) -> str:\n",
    "    \"\"\"Categorize a query based on keyword matching.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Define category keywords\n",
    "    categories = {\n",
    "        \"Publication Search\": [\"recent\", \"find\", \"search\", \"list\"],\n",
    "        \"Topic Summary\": [\"summarize\", \"summary\", \"what is\", \"explain\"],\n",
    "        \"Author Information\": [\"who\", \"author\", \"researcher\"],\n",
    "        \"Methodology\": [\"how\", \"method\", \"approach\"],\n",
    "        \"Citation Request\": [\"cite\", \"reference\", \"citation\"],\n",
    "    }\n",
    "\n",
    "    # Check each category\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in text_lower for word in keywords):\n",
    "            return category\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def _extract_query_texts(queries_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Extract query text from payload of messageSubmitted events.\"\"\"\n",
    "    query_list = []\n",
    "    for _, row in queries_df.iterrows():\n",
    "        payload = row.get(\"payload\", {})\n",
    "        if isinstance(payload, dict):\n",
    "            query_text = payload.get(\"message\", \"\")\n",
    "            if query_text:\n",
    "                query_list.append(query_text)\n",
    "    return query_list\n",
    "\n",
    "\n",
    "def _create_mock_query_data() -> pd.DataFrame:\n",
    "    \"\"\"Create demonstration data for query categorization.\"\"\"\n",
    "    query_categories = {\n",
    "        \"Publication Search\": [\"recent\", \"find\", \"search\", \"papers on\"],\n",
    "        \"Topic Summary\": [\"summarize\", \"what is\", \"explain\", \"overview\"],\n",
    "        \"Author Information\": [\"who is\", \"author\", \"researcher\", \"publications by\"],\n",
    "        \"Methodology\": [\"how to\", \"method\", \"approach\", \"technique\"],\n",
    "        \"Citation Request\": [\"cite\", \"reference\", \"bibliography\"],\n",
    "        \"Other\": [\"help\", \"can you\", \"tell me\"],\n",
    "    }\n",
    "\n",
    "    query_data = []\n",
    "    for category in query_categories:\n",
    "        count = np.random.randint(5, 30)\n",
    "        query_data.append({\"category\": category, \"count\": count})\n",
    "\n",
    "    return pd.DataFrame(query_data)\n",
    "\n",
    "\n",
    "def _categorize_queries(query_list: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Categorize a list of queries and return counts.\"\"\"\n",
    "    categories = [_categorize_query(q) for q in query_list]\n",
    "    query_df = pd.DataFrame({\"category\": categories})\n",
    "    query_df = query_df[\"category\"].value_counts().reset_index()\n",
    "    query_df.columns = [\"category\", \"count\"]\n",
    "    return query_df\n",
    "\n",
    "\n",
    "def _plot_query_bar_chart(query_df: pd.DataFrame, ax: plt.Axes) -> None:\n",
    "    \"\"\"Create horizontal bar chart for query categories.\"\"\"\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(query_df)))\n",
    "    ax.barh(query_df[\"category\"], query_df[\"count\"], color=colors, edgecolor=\"black\")\n",
    "\n",
    "    # Add value labels\n",
    "    max_count = query_df[\"count\"].max()\n",
    "    for i, (cat, count) in enumerate(zip(query_df[\"category\"], query_df[\"count\"])):\n",
    "        percentage = count / query_df[\"count\"].sum() * 100\n",
    "        ax.text(\n",
    "            count + max_count * 0.01,\n",
    "            i,\n",
    "            f\"{count} ({percentage:.1f}%)\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Number of Queries\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Query Category\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Query Type Classification: Main Use Cases\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "\n",
    "def plot_query_type_classification(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a bar chart showing query categories and subcategories.\n",
    "\n",
    "    Assumes messageSubmitted events contain query information.\n",
    "    \"\"\"\n",
    "    # Filter for query events\n",
    "    queries = events_df[events_df[\"eventType\"] == \"messageSubmitted\"].copy()\n",
    "\n",
    "    if len(queries) == 0:\n",
    "        print(\"No query events found\")\n",
    "        return\n",
    "\n",
    "    # Extract query text from payload\n",
    "    query_list = _extract_query_texts(queries)\n",
    "\n",
    "    if not query_list:\n",
    "        print(\"No query text found, creating demonstration data...\")\n",
    "        query_df = _create_mock_query_data()\n",
    "    else:\n",
    "        query_df = _categorize_queries(query_list)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    _plot_query_bar_chart(query_df, ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUERY TYPE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(query_df.to_string(index=False))\n",
    "    print(f\"\\nTotal queries: {query_df['count'].sum()}\")\n",
    "\n",
    "\n",
    "plot_query_type_classification(\n",
    "    events_df, save_path=\"viz3_query_type_classification.png\"\n",
    ")\n",
    "\n",
    "plot_query_type_classification(\n",
    "    events_df, save_path=\"viz3_query_type_classification.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: Session Depth Distribution\n",
    "Shows engagement levels through interaction counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_session_depth_distribution(events_df, save_path=None):\n",
    "    \"\"\"Create histogram showing distribution of interactions per session.\"\"\"\n",
    "    # Count events per session\n",
    "    session_stats_df = (\n",
    "        events_df.groupby(\"sessionId\").size().reset_index(name=\"event_count\")\n",
    "    )\n",
    "\n",
    "    # Cap at 21+ for better visualization\n",
    "    capped_counts = session_stats_df[\"event_count\"].clip(upper=21)\n",
    "\n",
    "    # Create histogram\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Left: Histogram with 21+ bin\n",
    "    ax1.hist(\n",
    "        capped_counts,\n",
    "        bins=range(1, 23),\n",
    "        align=\"left\",\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2,\n",
    "    )\n",
    "    ax1.set_title(\"Distribution of Events per Session\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.set_xlabel(\"Number of Events\", fontsize=11, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Number of Sessions\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    # Set x-ticks and relabel the last one as \"21+\"\n",
    "    xticks = list(range(1, 22))\n",
    "    labels = [str(x) if x < 21 else \"21+\" for x in xticks]\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Right: Box plot with statistics\n",
    "    ax2.boxplot(\n",
    "        session_stats_df[\"event_count\"],\n",
    "        vert=True,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightcoral\", alpha=0.7),\n",
    "        medianprops=dict(color=\"darkred\", linewidth=2),\n",
    "        whiskerprops=dict(color=\"black\", linewidth=1.5),\n",
    "        capprops=dict(color=\"black\", linewidth=1.5),\n",
    "    )\n",
    "\n",
    "    ax2.set_title(\"Session Depth Statistics\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Number of Events per Session\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_xticklabels([\"All Sessions\"])\n",
    "    ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add statistics as text\n",
    "    stats = session_stats_df[\"event_count\"].describe()\n",
    "    stats_text = f\"\"\"Mean: {stats[\"mean\"]:.1f}\n",
    "Median: {stats[\"50%\"]:.0f}\n",
    "Std: {stats[\"std\"]:.1f}\n",
    "Q1: {stats[\"25%\"]:.0f}\n",
    "Q3: {stats[\"75%\"]:.0f}\n",
    "Max: {stats[\"max\"]:.0f}\"\"\"\n",
    "\n",
    "    ax2.text(\n",
    "        1.15,\n",
    "        stats[\"75%\"],\n",
    "        stats_text,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"center\",\n",
    "    )\n",
    "\n",
    "    plt.suptitle(\"Session Engagement Analysis\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed statistics\n",
    "    print(\"SESSION DEPTH STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total sessions: {len(session_stats_df)}\")\n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(stats)\n",
    "    print(\"\\nEngagement categories:\")\n",
    "    print(\n",
    "        f\"  Low (1-5 events): {(session_stats_df['event_count'] <= 5).sum()} sessions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Medium (6-15 events): {((session_stats_df['event_count'] > 5) & (session_stats_df['event_count'] <= 15)).sum()} sessions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High (16+ events): {(session_stats_df['event_count'] > 15).sum()} sessions\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_session_depth_distribution(\n",
    "    events_df, save_path=\"viz4_session_depth_distribution.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: Response Quality Metrics Dashboard\n",
    "Quantifies system performance across key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_response_quality_dashboard(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a multi-panel dashboard showing key quality metrics.\n",
    "\n",
    "    Includes: citation accuracy, user satisfaction, response completeness.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Mock data (replace with actual metrics when available)\n",
    "    metrics = {\n",
    "        \"Citation Accuracy\": 0.92,\n",
    "        \"User Satisfaction\": 0.78,\n",
    "        \"Response Completeness\": 0.85,\n",
    "        \"Hallucination Rate\": 0.08,\n",
    "        \"Source Coverage\": 0.88,\n",
    "        \"Response Time (avg)\": 2.3,  # seconds\n",
    "    }\n",
    "\n",
    "    # Extract feedback events\n",
    "    feedback_events = events_df[events_df[\"eventType\"].isin([\"thumbsUp\", \"thumbsDown\"])]\n",
    "    if len(feedback_events) > 0:\n",
    "        thumbs_up = (feedback_events[\"eventType\"] == \"thumbsUp\").sum()\n",
    "        thumbs_down = (feedback_events[\"eventType\"] == \"thumbsDown\").sum()\n",
    "        total_feedback = thumbs_up + thumbs_down\n",
    "        if total_feedback > 0:\n",
    "            metrics[\"User Satisfaction\"] = thumbs_up / total_feedback\n",
    "\n",
    "    # Panel 1: Overall Score Gauge\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    overall_score = np.mean(\n",
    "        [\n",
    "            metrics[\"Citation Accuracy\"],\n",
    "            metrics[\"User Satisfaction\"],\n",
    "            metrics[\"Response Completeness\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create gauge\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    r = 1\n",
    "    ax1.plot(r * np.cos(theta), r * np.sin(theta), \"k-\", linewidth=3)\n",
    "    ax1.fill_between(\n",
    "        r * np.cos(theta), 0, r * np.sin(theta), alpha=0.2, color=\"lightblue\"\n",
    "    )\n",
    "\n",
    "    # Add colored segments\n",
    "    colors = [\"red\", \"orange\", \"yellow\", \"lightgreen\", \"green\"]\n",
    "    segments = 5\n",
    "    for i in range(segments):\n",
    "        theta_seg = np.linspace(i * np.pi / segments, (i + 1) * np.pi / segments, 20)\n",
    "        ax1.fill_between(\n",
    "            r * np.cos(theta_seg), 0, r * np.sin(theta_seg), alpha=0.3, color=colors[i]\n",
    "        )\n",
    "\n",
    "    # Add needle\n",
    "    needle_angle = overall_score * np.pi\n",
    "    ax1.plot(\n",
    "        [0, r * 0.9 * np.cos(needle_angle)],\n",
    "        [0, r * 0.9 * np.sin(needle_angle)],\n",
    "        \"r-\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    ax1.plot(0, 0, \"ro\", markersize=12)\n",
    "\n",
    "    # Labels\n",
    "    ax1.text(\n",
    "        0,\n",
    "        -0.3,\n",
    "        f\"Overall Quality Score\\n{overall_score:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax1.text(-1, 0, \"0%\", ha=\"right\", va=\"center\", fontsize=10)\n",
    "    ax1.text(1, 0, \"100%\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "    ax1.set_xlim(-1.3, 1.3)\n",
    "    ax1.set_ylim(-0.5, 1.3)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_aspect(\"equal\")\n",
    "\n",
    "    # Panel 2: Individual Metrics Bars\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    metric_names = [\n",
    "        \"Citation\\nAccuracy\",\n",
    "        \"User\\nSatisfaction\",\n",
    "        \"Response\\nCompleteness\",\n",
    "        \"Source\\nCoverage\",\n",
    "    ]\n",
    "    metric_values = [\n",
    "        metrics[\"Citation Accuracy\"],\n",
    "        metrics[\"User Satisfaction\"],\n",
    "        metrics[\"Response Completeness\"],\n",
    "        metrics[\"Source Coverage\"],\n",
    "    ]\n",
    "\n",
    "    colors_bars = [\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#06A77D\"]\n",
    "    bars = ax2.barh(metric_names, metric_values, color=colors_bars, edgecolor=\"black\")\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, metric_values)):\n",
    "        ax2.text(\n",
    "            val + 0.02, i, f\"{val:.1%}\", va=\"center\", fontweight=\"bold\", fontsize=11\n",
    "        )\n",
    "\n",
    "    ax2.set_xlim(0, 1.15)\n",
    "    ax2.set_xlabel(\"Score\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_title(\"Key Performance Metrics\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # Panel 3: Feedback Distribution\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    if len(feedback_events) > 0:\n",
    "        feedback_counts = feedback_events[\"eventType\"].value_counts()\n",
    "        colors_pie = [\"#06A77D\", \"#D62828\"]\n",
    "        ax3.pie(\n",
    "            feedback_counts.values,\n",
    "            labels=[\"👍 Positive\", \"👎 Negative\"],\n",
    "            autopct=\"%1.1f%%\",\n",
    "            colors=colors_pie,\n",
    "            startangle=90,\n",
    "            textprops={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    "        )\n",
    "        ax3.set_title(\n",
    "            f\"User Feedback\\n(n={len(feedback_events)})\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "    else:\n",
    "        ax3.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No feedback\\ndata available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        ax3.axis(\"off\")\n",
    "\n",
    "    # Panel 4: Hallucination Rate\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    hall_rate = metrics[\"Hallucination Rate\"]\n",
    "    colors_gauge = [\n",
    "        \"green\" if hall_rate < 0.1 else \"orange\" if hall_rate < 0.2 else \"red\"\n",
    "    ]\n",
    "    ax4.bar([\"Hallucination\\nRate\"], [hall_rate], color=colors_gauge, edgecolor=\"black\")\n",
    "    ax4.axhline(y=0.1, color=\"orange\", linestyle=\"--\", linewidth=2, label=\"Warning\")\n",
    "    ax4.axhline(y=0.2, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Critical\")\n",
    "    ax4.text(\n",
    "        0,\n",
    "        hall_rate + 0.02,\n",
    "        f\"{hall_rate:.1%}\",\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax4.set_ylim(0, 0.3)\n",
    "    ax4.set_ylabel(\"Rate\", fontsize=10, fontweight=\"bold\")\n",
    "    ax4.set_title(\"Hallucination Rate\", fontsize=11, fontweight=\"bold\")\n",
    "    ax4.legend(loc=\"upper right\", fontsize=8)\n",
    "    ax4.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Panel 5: Response Time\n",
    "    ax5 = fig.add_subplot(gs[2, 2])\n",
    "    resp_time = metrics[\"Response Time (avg)\"]\n",
    "    color_time = \"green\" if resp_time < 3 else \"orange\" if resp_time < 5 else \"red\"\n",
    "    ax5.bar([\"Avg Response\\nTime\"], [resp_time], color=color_time, edgecolor=\"black\")\n",
    "    ax5.text(\n",
    "        0,\n",
    "        resp_time + 0.2,\n",
    "        f\"{resp_time:.1f}s\",\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax5.set_ylabel(\"Seconds\", fontsize=10, fontweight=\"bold\")\n",
    "    ax5.set_title(\"Response Time\", fontsize=11, fontweight=\"bold\")\n",
    "    ax5.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Response Quality Metrics Dashboard\", fontsize=16, fontweight=\"bold\", y=0.98\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"QUALITY METRICS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for metric, value in metrics.items():\n",
    "        if \"Time\" in metric:\n",
    "            print(f\"{metric}: {value:.2f}s\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.1%}\")\n",
    "\n",
    "\n",
    "plot_response_quality_dashboard(\n",
    "    events_df, save_path=\"viz5_quality_metrics_dashboard.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 6: Cost Analysis Over Time\n",
    "Tracks financial sustainability with component breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_analysis(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create stacked area chart showing costs over time by component.\n",
    "\n",
    "    Assumes cost information is tracked in payload or can be estimated.\n",
    "    \"\"\"\n",
    "    # Generate mock cost data (replace with actual cost tracking)\n",
    "    dates = pd.date_range(\n",
    "        start=events_df[\"timestamp\"].min().date(),\n",
    "        end=events_df[\"timestamp\"].max().date(),\n",
    "        freq=\"D\",\n",
    "    )\n",
    "\n",
    "    # Estimate costs based on activity\n",
    "    daily_activity = events_df.groupby(events_df[\"timestamp\"].dt.date).size()\n",
    "\n",
    "    cost_data = []\n",
    "    for date in dates:\n",
    "        activity = daily_activity.get(date.date(), 0)\n",
    "        # Mock cost calculation (adjust based on actual pricing)\n",
    "        llm_cost = activity * 0.002  # $0.002 per API call estimate\n",
    "        hosting_cost = 6.67  # Daily fraction of monthly hosting (~$200/month)\n",
    "        storage_cost = 0.50  # Daily storage cost\n",
    "\n",
    "        cost_data.append(\n",
    "            {\n",
    "                \"date\": date,\n",
    "                \"LLM API\": llm_cost,\n",
    "                \"Hosting\": hosting_cost,\n",
    "                \"Storage\": storage_cost,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    cost_df = pd.DataFrame(cost_data)\n",
    "    cost_df[\"Total\"] = cost_df[[\"LLM API\", \"Hosting\", \"Storage\"]].sum(axis=1)\n",
    "    cost_df[\"Cumulative\"] = cost_df[\"Total\"].cumsum()\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Top: Stacked area chart of daily costs\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        0,\n",
    "        cost_df[\"LLM API\"],\n",
    "        label=\"LLM API\",\n",
    "        alpha=0.7,\n",
    "        color=\"#E63946\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"LLM API\"],\n",
    "        cost_df[\"LLM API\"] + cost_df[\"Hosting\"],\n",
    "        label=\"Hosting\",\n",
    "        alpha=0.7,\n",
    "        color=\"#F1FAEE\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"LLM API\"] + cost_df[\"Hosting\"],\n",
    "        cost_df[\"Total\"],\n",
    "        label=\"Storage\",\n",
    "        alpha=0.7,\n",
    "        color=\"#A8DADC\",\n",
    "    )\n",
    "\n",
    "    ax1.set_title(\"Daily Cost Breakdown by Component\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Cost (€)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax1.legend(loc=\"upper left\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    # Bottom: Cumulative cost line\n",
    "    ax2.plot(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"Cumulative\"],\n",
    "        linewidth=3,\n",
    "        color=\"#1D3557\",\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax2.fill_between(\n",
    "        cost_df[\"date\"], 0, cost_df[\"Cumulative\"], alpha=0.3, color=\"#457B9D\"\n",
    "    )\n",
    "\n",
    "    # Add milestone markers\n",
    "    total_cost = cost_df[\"Cumulative\"].iloc[-1]\n",
    "    ax2.axhline(y=total_cost, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "    ax2.text(\n",
    "        cost_df[\"date\"].iloc[-1],\n",
    "        total_cost + 10,\n",
    "        f\"Total: €{total_cost:.2f}\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"yellow\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "    ax2.set_title(\"Cumulative Cost Over Time\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"Date\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Cumulative Cost (€)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Cost Analysis: Financial Sustainability Tracking\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.995,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print cost summary\n",
    "    print(\"COST ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Period: {cost_df['date'].min().date()} to {cost_df['date'].max().date()}\")\n",
    "    print(\"\\nTotal costs:\")\n",
    "    print(f\"  LLM API: €{cost_df['LLM API'].sum():.2f}\")\n",
    "    print(f\"  Hosting: €{cost_df['Hosting'].sum():.2f}\")\n",
    "    print(f\"  Storage: €{cost_df['Storage'].sum():.2f}\")\n",
    "    print(f\"  TOTAL: €{cost_df['Total'].sum():.2f}\")\n",
    "    print(f\"\\nAverage daily cost: €{cost_df['Total'].mean():.2f}\")\n",
    "    print(f\"Projected monthly cost: €{cost_df['Total'].mean() * 30:.2f}\")\n",
    "\n",
    "\n",
    "plot_cost_analysis(events_df, save_path=\"viz6_cost_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 8: Citation Network Visualization\n",
    "Reveals which research is most accessed and co-citation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_mock_citation_data() -> tuple[dict, list]:\n",
    "    \"\"\"\n",
    "    Create demonstration citation data.\n",
    "\n",
    "    # TODO: extract from messageReceived events with citations\n",
    "    \"\"\"\n",
    "    publications = [\n",
    "        \"Godard2021_Precautionary\",\n",
    "        \"Bouleau2008_Scientists\",\n",
    "        \"Bouleau2013_Architectural\",\n",
    "        \"Guivarch2019_Climate\",\n",
    "        \"Cassen2015_Water\",\n",
    "        \"Bibas2016_LowCarbon\",\n",
    "        \"Lefèvre2018_Transport\",\n",
    "        \"Rozenberg2019_Infrastructure\",\n",
    "        \"Waisman2020_Pathways\",\n",
    "        \"Hourcade2021_Carbon\",\n",
    "    ]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    citation_counts = {pub: np.random.randint(5, 50) for pub in publications}\n",
    "\n",
    "    return citation_counts, publications\n",
    "\n",
    "\n",
    "def _build_citation_graph(publications: list, citation_counts: dict):\n",
    "    \"\"\"Build NetworkX graph with citation nodes and co-citation edges.\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with citation counts\n",
    "    for pub, count in citation_counts.items():\n",
    "        G.add_node(pub, citations=count)\n",
    "\n",
    "    # Add edges for co-citations (publications cited together)\n",
    "    for i, pub1 in enumerate(publications[:-1]):\n",
    "        for pub2 in publications[i + 1 :]:\n",
    "            # Random co-citation strength\n",
    "            if np.random.random() > 0.6:  # 40% chance of connection\n",
    "                weight = np.random.randint(1, 10)\n",
    "                G.add_edge(pub1, pub2, weight=weight)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def _draw_citation_network(G, ax: plt.Axes, publications: list) -> None:\n",
    "    \"\"\"Draw the citation network graph on the given axes.\"\"\"\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "    # Node sizes based on citation counts\n",
    "    node_sizes = [G.nodes[node][\"citations\"] * 20 for node in G.nodes()]\n",
    "\n",
    "    # Edge widths based on co-citation strength\n",
    "    edge_widths = [G[u][v][\"weight\"] * 0.5 for u, v in G.edges()]\n",
    "\n",
    "    # Draw network components\n",
    "    nx.draw_networkx_nodes(\n",
    "        G,\n",
    "        pos,\n",
    "        ax=ax,\n",
    "        node_size=node_sizes,\n",
    "        node_color=\"lightblue\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax, width=edge_widths, alpha=0.6, edge_color=\"gray\"\n",
    "    )\n",
    "\n",
    "    # Simplified labels (just first author and year)\n",
    "    labels = {pub: pub.split(\"_\")[0] for pub in publications}\n",
    "    nx.draw_networkx_labels(G, pos, labels, ax=ax, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Publication Citation Network\\n(Node size = citation frequency)\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def _draw_citation_bar_chart(\n",
    "    citation_counts: dict, ax: plt.Axes, top_n: int = 10\n",
    ") -> None:\n",
    "    \"\"\"Draw bar chart of top cited publications.\"\"\"\n",
    "    sorted_pubs = sorted(citation_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_pubs = sorted_pubs[:top_n]\n",
    "\n",
    "    pub_names = [p[0].replace(\"_\", \"\\n\") for p in top_pubs]\n",
    "    pub_counts = [p[1] for p in top_pubs]\n",
    "\n",
    "    colors_bar = plt.cm.viridis(np.linspace(0.3, 0.9, len(pub_names)))\n",
    "    bars = ax.barh(pub_names, pub_counts, color=colors_bar, edgecolor=\"black\")\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, pub_counts):\n",
    "        ax.text(\n",
    "            count + 1,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            str(count),\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Number of Citations\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\"Top 10 Most Cited Publications\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "\n",
    "def _print_citation_statistics(G, citation_counts: dict) -> None:\n",
    "    \"\"\"Print citation network statistics.\"\"\"\n",
    "    sorted_pubs = sorted(citation_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"CITATION NETWORK STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total publications: {G.number_of_nodes()}\")\n",
    "    print(f\"Total co-citation relationships: {G.number_of_edges()}\")\n",
    "    print(f\"Network density: {nx.density(G):.3f}\")\n",
    "    print(\n",
    "        f\"\\nMost cited publication: {sorted_pubs[0][0]} ({sorted_pubs[0][1]} citations)\"\n",
    "    )\n",
    "\n",
    "    # Calculate centrality\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    most_central = max(centrality.items(), key=lambda x: x[1])\n",
    "    print(\n",
    "        f\"Most connected publication: {most_central[0]} (centrality: {most_central[1]:.3f})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_citation_network(events_df, save_path=None):\n",
    "    \"\"\"Create network graph showing most-cited publications and co-citation patterns.\"\"\"\n",
    "    citation_counts, publications = _create_mock_citation_data()\n",
    "\n",
    "    # Create graph\n",
    "    G = _build_citation_graph(publications, citation_counts)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "    # Left: Network graph\n",
    "    _draw_citation_network(G, ax1, publications)\n",
    "\n",
    "    # Right: Bar chart of top cited publications\n",
    "    _draw_citation_bar_chart(citation_counts, ax2)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Citation Analysis: Research Impact and Connections\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print network statistics\n",
    "    _print_citation_statistics(G, citation_counts)\n",
    "\n",
    "\n",
    "plot_citation_network(events_df, save_path=\"viz8_citation_network.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 11: User Journey Funnel\n",
    "Identifies friction points in user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Compute helpers ----------\n",
    "\n",
    "\n",
    "def _funnel_counts(events_df: pd.DataFrame) -> tuple[list[str], list[int]]:\n",
    "    stages = [\n",
    "        \"Landing\",\n",
    "        \"Session Start\",\n",
    "        \"First Query\",\n",
    "        \"Multiple\\nInteractions\\n(3+)\",\n",
    "        \"Feedback\\nProvided\",\n",
    "    ]\n",
    "    total_sessions = int(events_df[\"sessionId\"].nunique())\n",
    "    started = int(events_df.query(\"eventType == 'sessionStart'\")[\"sessionId\"].nunique())\n",
    "    queried = int(\n",
    "        events_df.query(\"eventType == 'messageSubmitted'\")[\"sessionId\"].nunique()\n",
    "    )\n",
    "\n",
    "    msg_counts = (\n",
    "        events_df.query(\"eventType == 'messageSubmitted'\").groupby(\"sessionId\").size()\n",
    "    )\n",
    "    engaged = int((msg_counts >= 3).sum())\n",
    "\n",
    "    feedback = int(\n",
    "        events_df.query(\"eventType in ['thumbsUp', 'thumbsDown']\")[\n",
    "            \"sessionId\"\n",
    "        ].nunique()\n",
    "    )\n",
    "\n",
    "    counts = [total_sessions, started, queried, engaged, feedback]\n",
    "    return stages, counts\n",
    "\n",
    "\n",
    "def _rates(counts: list[int]) -> tuple[list[float], list[float]]:\n",
    "    if not counts or counts[0] <= 0:\n",
    "        return [0.0] * len(counts), [0.0] * (len(counts) - 1)\n",
    "\n",
    "    base = counts[0]\n",
    "    conv = [100.0] + [c / base * 100.0 for c in counts[1:]]\n",
    "    drop = [\n",
    "        ((counts[i] - counts[i + 1]) / counts[i] * 100.0) if counts[i] > 0 else 0.0\n",
    "        for i in range(len(counts) - 1)\n",
    "    ]\n",
    "    return conv, drop\n",
    "\n",
    "\n",
    "# ---------- Plot helpers ----------\n",
    "\n",
    "\n",
    "def _plot_funnel(\n",
    "    ax: plt.Axes, stages: list[str], counts: list[int], conv: list[float]\n",
    ") -> None:\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(stages)))\n",
    "    max_width = max(counts) if counts else 1\n",
    "    scale = 10.0\n",
    "\n",
    "    for i, (stage, count, color) in enumerate(zip(stages, counts, colors)):\n",
    "        width = (count / max_width) * scale if max_width else 0.0\n",
    "        left = (scale - width) / 2.0\n",
    "\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (left, i), width, 0.8, facecolor=color, edgecolor=\"black\", linewidth=2\n",
    "            )\n",
    "        )\n",
    "        ax.text(\n",
    "            scale / 2.0,\n",
    "            i + 0.4,\n",
    "            stage,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.text(\n",
    "            scale + 1.0,\n",
    "            i + 0.4,\n",
    "            f\"{count}\\n({conv[i]:.1f}%)\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0, scale + 4)\n",
    "    ax.set_ylim(-0.5, len(stages))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"User Journey Funnel\", fontsize=13, fontweight=\"bold\", pad=20)\n",
    "\n",
    "\n",
    "def _plot_conversion(ax: plt.Axes, stages: list[str], conv: list[float]) -> None:\n",
    "    ax.plot(\n",
    "        stages, conv, linewidth=3, marker=\"o\", markersize=12, markerfacecolor=\"white\"\n",
    "    )\n",
    "    for i, rate in enumerate(conv):\n",
    "        ax.text(\n",
    "            i, rate + 3, f\"{rate:.1f}%\", ha=\"center\", fontweight=\"bold\", fontsize=10\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"Conversion Rate (%)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Conversion Rate Through Funnel Stages\", fontsize=13, fontweight=\"bold\"\n",
    "    )\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    # Quality bands\n",
    "    ax.axhspan(0, 30, alpha=0.1)\n",
    "    ax.axhspan(30, 60, alpha=0.1)\n",
    "    ax.axhspan(60, 100, alpha=0.1)\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Poor\"),\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Fair\"),\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Good\"),\n",
    "        ],\n",
    "        loc=\"upper right\",\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- Reporting helper ----------\n",
    "\n",
    "\n",
    "def _print_funnel_report(\n",
    "    stages: list[str], counts: list[int], conv: list[float], drop: list[float]\n",
    ") -> None:\n",
    "    print(\"USER JOURNEY FUNNEL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nFunnel Statistics:\")\n",
    "    for i, (stage, count, rate) in enumerate(zip(stages, counts, conv), 1):\n",
    "        label = stage.replace(\"\\n\", \" \")\n",
    "        print(f\"\\n{i}. {label}:\\n   Count: {count}\\n   Conversion: {rate:.1f}%\")\n",
    "        if i - 1 < len(drop):\n",
    "            print(f\"   Drop-off to next stage: {drop[i - 1]:.1f}%\")\n",
    "\n",
    "    overall = conv[-1] if conv else 0.0\n",
    "    largest_drop = max(drop) if drop else 0.0\n",
    "    largest_drop_stage = (drop.index(largest_drop) + 1) if drop else 0\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"  Landing to Feedback: {overall:.1f}% conversion\")\n",
    "    print(f\"  Largest drop-off: {largest_drop:.1f}% at stage {largest_drop_stage}\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    if drop and drop[0] > 20:\n",
    "        print(\"  ⚠ High drop-off after landing — improve onboarding.\")\n",
    "    if len(drop) > 2 and drop[2] > 30:\n",
    "        print(\"  ⚠ High drop-off after first query — review response quality.\")\n",
    "    if overall < 20:\n",
    "        print(\"  ⚠ Low feedback rate — make feedback more prominent.\")\n",
    "\n",
    "\n",
    "# ---------- Orchestrator (low-complexity) ----------\n",
    "\n",
    "\n",
    "def plot_user_journey_funnel(\n",
    "    events_df: pd.DataFrame,\n",
    "    save_path: str | None = None,\n",
    "    *,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[plt.Figure, tuple[plt.Axes, plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Create funnel chart showing user progression through key stages.\n",
    "\n",
    "    Identifies drop-off points in the user journey.\n",
    "    \"\"\"\n",
    "    stages, counts = _funnel_counts(events_df)\n",
    "    conv, drop = _rates(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    _plot_funnel(ax1, stages, counts, conv)\n",
    "    _plot_conversion(ax2, stages, conv)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"User Journey Analysis: Conversion Funnel and Drop-off Points\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        _print_funnel_report(stages, counts, conv, drop)\n",
    "\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_journey_funnel(events_df, save_path=\"viz11_user_journey_funnel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 12: Topic Coverage Heat Map\n",
    "Identifies well-covered vs underserved research areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Data prep ----------\n",
    "\n",
    "\n",
    "def _default_themes() -> list[str]:\n",
    "    return [\n",
    "        \"Climate Policy\",\n",
    "        \"Energy Transition\",\n",
    "        \"Urban Planning\",\n",
    "        \"Transport\",\n",
    "        \"Circular Economy\",\n",
    "        \"Biodiversity\",\n",
    "        \"Water Management\",\n",
    "        \"Agriculture\",\n",
    "        \"Economic Modeling\",\n",
    "        \"Governance\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def _metric_labels() -> list[str]:\n",
    "    return [\n",
    "        \"Query\\nFrequency\",\n",
    "        \"Response\\nQuality\",\n",
    "        \"Citation\\nCoverage\",\n",
    "        \"User\\nSatisfaction\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def _make_mock_coverage(\n",
    "    themes: list[str], metrics: list[str], seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a mock coverage matrix scaled to [2, 10] with a few hand-tuned patterns.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = rng.random((len(themes), len(metrics))) * 8 + 2\n",
    "\n",
    "    theme_idx = {t: i for i, t in enumerate(themes)}\n",
    "    metric_idx = {m: j for j, m in enumerate(metrics)}\n",
    "\n",
    "    data[theme_idx[\"Climate Policy\"], metric_idx[\"Query\\nFrequency\"]] = 9.5\n",
    "    data[theme_idx[\"Energy Transition\"], metric_idx[\"Query\\nFrequency\"]] = 9.0\n",
    "    data[theme_idx[\"Water Management\"], metric_idx[\"Citation\\nCoverage\"]] = 8.5\n",
    "    data[theme_idx[\"Biodiversity\"], metric_idx[\"Citation\\nCoverage\"]] = 6.0\n",
    "\n",
    "    return pd.DataFrame(data, columns=_metric_labels(), index=themes)\n",
    "\n",
    "\n",
    "def _compute_summary(df: pd.DataFrame) -> tuple[pd.Series, pd.DataFrame]:\n",
    "    avg_by_theme = df.mean(axis=1)\n",
    "    ranked = df.copy()\n",
    "    ranked[\"Average\"] = avg_by_theme\n",
    "    return avg_by_theme, ranked\n",
    "\n",
    "\n",
    "# ---------- Plot helpers ----------\n",
    "\n",
    "\n",
    "def _plot_heatmap(ax: plt.Axes, df: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\"Heatmap with inline annotations and grid.\"\"\"\n",
    "    im = ax.imshow(df.values, cmap=\"RdYlGn\", aspect=\"auto\", vmin=2, vmax=10)\n",
    "\n",
    "    ax.set_xticks(np.arange(df.shape[1]))\n",
    "    ax.set_yticks(np.arange(df.shape[0]))\n",
    "    ax.set_xticklabels(list(df.columns), fontsize=11)\n",
    "    ax.set_yticklabels(list(df.index), fontsize=11)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            val = float(df.iat[i, j])\n",
    "            text_color = \"white\" if val < 6 else \"black\"\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{val:.1f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    ax.set_title(title, fontsize=13, fontweight=\"bold\", pad=15)\n",
    "    ax.set_xticks(np.arange(n_cols) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(n_rows) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle=\"-\", linewidth=1.5)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Score (2-10)\", rotation=270, labelpad=20, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "def _plot_radar(ax: plt.Axes, themes: list[str], avg_scores: list[float]) -> None:\n",
    "    \"\"\"Polar radar of average coverage by theme.\"\"\"\n",
    "    n = len(avg_scores)\n",
    "    angles = np.linspace(0, 2 * np.pi, n, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    scores = avg_scores + avg_scores[:1]\n",
    "\n",
    "    ax.plot(angles, scores, \"o-\", linewidth=3, markersize=8)\n",
    "    ax.fill(angles, scores, alpha=0.25)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_yticks([2, 4, 6, 8, 10])\n",
    "    ax.set_yticklabels([\"2\", \"4\", \"6\", \"8\", \"10\"], fontsize=9)\n",
    "\n",
    "    ax.plot(angles, [6] * len(angles), \"--\", linewidth=2, label=\"Target (6.0)\")\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(themes, fontsize=9)\n",
    "    ax.set_title(\n",
    "        \"Average Coverage Score by Research Theme\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.05))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# ---------- Orchestrator ----------\n",
    "\n",
    "\n",
    "def plot_topic_coverage_heatmap(\n",
    "    events_df: pd.DataFrame | None = None,\n",
    "    *,\n",
    "    themes: list[str] | None = None,\n",
    "    save_path: str | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[plt.Figure, tuple[plt.Axes, plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Create a matrix heatmap + radar showing CIRED research themes coverage.\n",
    "\n",
    "    Shows query frequency and response quality by research area.\n",
    "    \"\"\"\n",
    "    themes = themes or _default_themes()\n",
    "    metrics = _metric_labels()\n",
    "    coverage_df = _make_mock_coverage(themes, metrics)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax_left = plt.subplot(121)\n",
    "    ax_right = plt.subplot(122, projection=\"polar\")\n",
    "\n",
    "    _plot_heatmap(ax_left, coverage_df, \"Topic Coverage Heat Map by Metrics\")\n",
    "    avg_by_theme, ranked = _compute_summary(coverage_df)\n",
    "    _plot_radar(ax_right, themes, avg_by_theme.to_list())\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Research Topic Coverage Analysis: Identifying Gaps and Strengths\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        _print_coverage_analysis(coverage_df, ranked)\n",
    "\n",
    "    return fig, (ax_left, ax_right)\n",
    "\n",
    "\n",
    "# ---------- Reporting ----------\n",
    "\n",
    "\n",
    "def _print_coverage_analysis(coverage_df: pd.DataFrame, ranked: pd.DataFrame) -> None:\n",
    "    \"\"\"Console summary of coverage levels and weak spots.\"\"\"\n",
    "    print(\"TOPIC COVERAGE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for metric in _metric_labels():\n",
    "        avg = float(coverage_df[metric].mean())\n",
    "        label = metric.replace(\"\\n\", \" \")\n",
    "        print(f\"  {label}: {avg:.2f} (avg across themes)\")\n",
    "\n",
    "    sorted_themes = ranked.sort_values(\"Average\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 3 Well-Covered Themes:\")\n",
    "    for i, (theme, row) in enumerate(sorted_themes.head(3).iterrows(), 1):\n",
    "        print(f\"  {i}. {theme}: {row['Average']:.2f}\")\n",
    "\n",
    "    print(\"\\nBottom 3 Underserved Themes:\")\n",
    "    for i, (theme, row) in enumerate(sorted_themes.tail(3).iterrows(), 1):\n",
    "        print(f\"  {i}. {theme}: {row['Average']:.2f}\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    weak = ranked[ranked[\"Average\"] < 6]\n",
    "    if not weak.empty:\n",
    "        print(f\"  ⚠ {len(weak)} theme(s) below target coverage (6.0):\")\n",
    "        for theme in weak.index:\n",
    "            print(\n",
    "                f\"    - {theme}: Consider adding more publications or improving indexing\"\n",
    "            )\n",
    "\n",
    "    for metric in _metric_labels():\n",
    "        low = coverage_df[coverage_df[metric] < 5]\n",
    "        if not low.empty:\n",
    "            label = metric.replace(\"\\n\", \" \")\n",
    "            print(f\"  ⚠ Low {label} for: {', '.join(low.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_coverage_heatmap(events_df, save_path=\"viz12_topic_coverage_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Export\n",
    "\n",
    "Generate a summary report and export all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(events_df):\n",
    "    \"\"\"Generate a comprehensive text summary of all analyses.\"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"CIRED.DIGITAL PROJECT - FINAL ANALYSIS REPORT\")\n",
    "\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\n",
    "        f\"Analysis period: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\"\n",
    "    )\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"1. OVERALL STATISTICS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Total events: {len(events_df):,}\")\n",
    "    report.append(f\"Unique sessions: {events_df['sessionId'].nunique():,}\")\n",
    "    report.append(\n",
    "        f\"Date range: {(events_df['timestamp'].max() - events_df['timestamp'].min()).days} days\"\n",
    "    )\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"2. KEY FINDINGS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\n[Add key findings based on the 12 visualizations above]\")\n",
    "    report.append(\"- User engagement patterns: [...]\")\n",
    "    report.append(\"- Most popular research areas: [...]\")\n",
    "    report.append(\"- Quality metrics performance: [...]\")\n",
    "    report.append(\"- Cost sustainability: [...]\")\n",
    "    report.append(\"- Environmental impact: [...]\")\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"3. RECOMMENDATIONS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\n[Add recommendations for:]\")\n",
    "    report.append(\"- System improvements\")\n",
    "    report.append(\"- User experience enhancements\")\n",
    "    report.append(\"- Cost optimization\")\n",
    "    report.append(\"- Content coverage expansion\")\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"END OF REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "\n",
    "    report_text = \"\\n\".join(report)\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"final_analysis_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(report_text)\n",
    "    print(\"\\n✓ Report saved to: final_analysis_report.txt\")\n",
    "    print(\"✓ All visualizations saved as PNG files (viz1_*.png through viz12_*.png)\")\n",
    "\n",
    "\n",
    "generate_summary_report(events_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
