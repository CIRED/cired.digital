{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web App Event Logs - Enhanced Analysis for Final Report\n",
    "\n",
    "This notebook analyzes web app event logs stored as JSON files in `reports/monitor-logs/YYYY/MM/DD/` structure.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Preparation\n",
    "2. 12 Key Visualizations for Final Report\n",
    "3. Additional Analysis\n",
    "\n",
    "Each JSON file contains:\n",
    "```json\n",
    "{\n",
    "  \"sessionId\": \"session_1754052978056_4i3x2c3bc1g3g4j2\",\n",
    "  \"timestamp\": \"20250801T125618127Z\",\n",
    "  \"eventType\": \"sessionStart\",\n",
    "  \"payload\": { ... },\n",
    "  \"server_context\": { ... }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "# Set up paths\n",
    "log_base_path = Path(\"../tests/fixtures/reports/monitor-logs\")\n",
    "print(f\"Base log path: {log_base_path}\")\n",
    "print(f\"Path exists: {log_base_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_log_files(base_path=\"monitor-logs\", min_date=\"20250705\"):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the log directory structure.\n",
    "\n",
    "    Handles the recursive YYYY/MM/DD/ folder structure.\n",
    "    Each JSON file contains: sessionId, timestamp, eventType, payload, server_context.\n",
    "\n",
    "    Args:\n",
    "        base_path: Path to monitor-logs directory\n",
    "        min_date: Only load files from this date onwards (format: YYYYMMDD, e.g., '20250705')\n",
    "\n",
    "    \"\"\"\n",
    "    all_events = []\n",
    "    file_count = 0\n",
    "    error_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Use glob to find all JSON files recursively\n",
    "    json_pattern = os.path.join(base_path, \"**/*.json\")\n",
    "    json_files = glob.glob(json_pattern, recursive=True)\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files total\\n\")\n",
    "    if min_date:\n",
    "        print(f\"Filtering for files from {min_date} onwards (July 5, 2025)\\n\")\n",
    "\n",
    "    # Load each file\n",
    "    for file_path in json_files:\n",
    "        # Filter by date if min_date is specified\n",
    "        if min_date:\n",
    "            # Extract date from path: monitor-logs/YYYY/MM/DD/...\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            if len(path_parts) >= 4:\n",
    "                try:\n",
    "                    year = path_parts[-4]\n",
    "                    month = path_parts[-3]\n",
    "                    day = path_parts[-2]\n",
    "                    file_date = f\"{year}{month}{day}\"\n",
    "                    if file_date < min_date:\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                event_data = json.load(f)\n",
    "\n",
    "            # Validate required fields\n",
    "            if all(\n",
    "                key in event_data for key in [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "            ):\n",
    "                all_events.append(event_data)\n",
    "                file_count += 1\n",
    "            else:\n",
    "                error_count += 1\n",
    "\n",
    "        except (OSError, json.JSONDecodeError):\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Successfully loaded: {file_count} files\")\n",
    "    print(f\"Skipped (before cutoff): {skipped_count} files\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors encountered: {error_count} files\\n\")\n",
    "\n",
    "    return all_events\n",
    "\n",
    "\n",
    "# Load all events (from July 5, 2025 onwards)\n",
    "all_events = load_all_log_files(min_date=\"20250705\")\n",
    "\n",
    "print(f\"\\nTotal events loaded: {len(all_events)}\")\n",
    "\n",
    "# Create DataFrame immediately\n",
    "events_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Parse timestamp to datetime\n",
    "events_df[\"timestamp\"] = pd.to_datetime(\n",
    "    events_df[\"timestamp\"], format=\"%Y%m%dT%H%M%S%fZ\"\n",
    ")\n",
    "\n",
    "# Explode server_context into separate columns\n",
    "if \"server_context\" in events_df.columns:\n",
    "    server_context_df = pd.json_normalize(events_df[\"server_context\"].tolist())\n",
    "    # Prefix columns to avoid conflicts\n",
    "    server_context_df.columns = [\"server_\" + col for col in server_context_df.columns]\n",
    "    events_df = pd.concat([events_df, server_context_df], axis=1)\n",
    "    print(f\"\\nExpanded {len(server_context_df.columns)} server_context fields\")\n",
    "\n",
    "# Add derived columns for analysis\n",
    "events_df[\"date\"] = events_df[\"timestamp\"].dt.date\n",
    "events_df[\"hour\"] = events_df[\"timestamp\"].dt.hour\n",
    "events_df[\"day_of_week\"] = events_df[\"timestamp\"].dt.day_name()\n",
    "\n",
    "print(f\"\\nDataFrame created with shape: {events_df.shape}\")\n",
    "print(f\"Columns: {events_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation - Generation of supplementary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract query text from messageSubmitted events\n",
    "def extract_query_from_payload(payload):\n",
    "    \"\"\"Extract query text from request event payload.\"\"\"\n",
    "    if isinstance(payload, dict):\n",
    "        # Try different possible keys\n",
    "        if \"query\" in payload:\n",
    "            return payload[\"query\"]\n",
    "        if \"message\" in payload:\n",
    "            return payload[\"message\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Add query text column for request events\n",
    "events_df[\"query_text\"] = events_df.apply(\n",
    "    lambda row: extract_query_from_payload(row[\"payload\"])\n",
    "    if row[\"eventType\"] == \"request\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Extract user profile information from sessionStart events\n",
    "def extract_profile_field(payload, field):\n",
    "    \"\"\"Extract profile field from sessionStart payload.\"\"\"\n",
    "    if isinstance(payload, dict):\n",
    "        profile = payload.get(\"profile\", {})\n",
    "        if isinstance(profile, dict):\n",
    "            return profile.get(field, None)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Add profile columns\n",
    "events_df[\"user_organization\"] = events_df.apply(\n",
    "    lambda row: extract_profile_field(row[\"payload\"], \"organization\")\n",
    "    if row[\"eventType\"] == \"sessionStart\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "events_df[\"user_knowledge\"] = events_df.apply(\n",
    "    lambda row: extract_profile_field(row[\"payload\"], \"knowledge\")\n",
    "    if row[\"eventType\"] == \"sessionStart\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "events_df[\"user_usage\"] = events_df.apply(\n",
    "    lambda row: extract_profile_field(row[\"payload\"], \"usage\")\n",
    "    if row[\"eventType\"] == \"sessionStart\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Extract model information from request events\n",
    "def extract_model_from_payload(payload):\n",
    "    \"\"\"Extract model name from request event payload.\"\"\"\n",
    "    if isinstance(payload, dict):\n",
    "        settings = payload.get(\"settings\", {})\n",
    "        if isinstance(settings, dict):\n",
    "            return settings.get(\"model\", None)\n",
    "    return None\n",
    "\n",
    "\n",
    "events_df[\"model_used\"] = events_df.apply(\n",
    "    lambda row: extract_model_from_payload(row[\"payload\"])\n",
    "    if row[\"eventType\"] == \"request\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Extract citations from response events\n",
    "def extract_citations_from_payload(payload):\n",
    "    \"\"\"Extract citations list from response event payload.\"\"\"\n",
    "    if isinstance(payload, dict):\n",
    "        response = payload.get(\"response\", {})\n",
    "        if isinstance(response, dict):\n",
    "            results = response.get(\"results\", {})\n",
    "            if isinstance(results, dict):\n",
    "                return results.get(\"citations\", [])\n",
    "    return []\n",
    "\n",
    "\n",
    "events_df[\"citations\"] = events_df.apply(\n",
    "    lambda row: extract_citations_from_payload(row[\"payload\"])\n",
    "    if row[\"eventType\"] == \"response\"\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Count citations per response\n",
    "events_df[\"citation_count\"] = events_df[\"citations\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(\"\\nAugmented DataFrame with computed columns:\")\n",
    "print(f\"  - query_text: {events_df['query_text'].notna().sum()} non-null values\")\n",
    "print(\n",
    "    f\"  - user_organization: {events_df['user_organization'].notna().sum()} non-null values\"\n",
    ")\n",
    "print(f\"  - model_used: {events_df['model_used'].notna().sum()} non-null values\")\n",
    "print(f\"  - citations: {events_df['citations'].notna().sum()} non-null values\")\n",
    "print(\n",
    "    f\"  - citation_count: {(events_df['citation_count'] > 0).sum()} responses with citations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total events:          {len(events_df):,}\")\n",
    "print(f\"Total unique sessions: {events_df['sessionId'].nunique():,}\")\n",
    "print(\n",
    "    f\"Avg events per session: {len(events_df) / events_df['sessionId'].nunique():.2f}\"\n",
    ")\n",
    "print(f\"\\nDate range: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\n",
    "\n",
    "# Show server context fields if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    print(\n",
    "        f\"\\nServer context fields: {', '.join([col.replace('server_', '') for col in server_cols])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events by Type\n",
    "\n",
    "Breakdown of event types in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "event_type_df = events_df[\"eventType\"].value_counts().reset_index()\n",
    "event_type_df.columns = [\"eventType\", \"count\"]\n",
    "event_type_df[\"percentage\"] = (\n",
    "    event_type_df[\"count\"] / event_type_df[\"count\"].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "print(event_type_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Events\n",
    "\n",
    "Peek at sample events from each type to understand data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    sample = events_df[events_df[\"eventType\"] == event_type].iloc[0].to_dict()\n",
    "    print(f\"\\n{event_type.upper()}:\")\n",
    "    print(json.dumps(sample, indent=2, default=str)[:600])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Overview\n",
    "\n",
    "Display key information about the DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataFrame shape: {events_df.shape}\")\n",
    "print(f\"\\nColumns: {events_df.columns.tolist()}\")\n",
    "print(\"\\nData types:\")\n",
    "print(events_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display_cols = [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "# Add server context columns if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    display_cols.extend(server_cols[:3])  # Show first 3 server fields\n",
    "print(events_df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payload Structure Analysis\n",
    "\n",
    "Analyze the structure of payload fields for each event type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PAYLOAD STRUCTURE BY EVENT TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract payload keys for each event type\n",
    "payload_structure = []\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    type_df = events_df[events_df[\"eventType\"] == event_type]\n",
    "    all_keys = set()\n",
    "    for payload in type_df[\"payload\"]:\n",
    "        if isinstance(payload, dict):\n",
    "            all_keys.update(payload.keys())\n",
    "\n",
    "    payload_structure.append(\n",
    "        {\n",
    "            \"eventType\": event_type,\n",
    "            \"count\": len(type_df),\n",
    "            \"payload_keys\": sorted(list(all_keys)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "payload_df = pd.DataFrame(payload_structure)\n",
    "for _, row in payload_df.iterrows():\n",
    "    print(f\"\\n{row['eventType']} ({row['count']} events):\")\n",
    "    print(f\"  Keys: {', '.join(row['payload_keys'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Visualizations for Final Report\n",
    "\n",
    "These visualizations are designed for inclusion in the final project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Session Activity Timeline\n",
    "Shows adoption patterns and activity peaks across project phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_session_activity_timeline(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a dual-axis line chart showing sessions and queries over time.\n",
    "\n",
    "    Includes phase markers (Alpha, Beta closed, Beta open).\n",
    "    \"\"\"\n",
    "    # Aggregate by date\n",
    "    daily_stats = (\n",
    "        events_df.groupby(\"date\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"sessionId\": \"nunique\",  # Unique sessions\n",
    "                \"eventType\": \"count\",  # Total events\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    daily_stats.columns = [\"date\", \"sessions\", \"events\"]\n",
    "\n",
    "    # Create figure with dual axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot sessions on left axis\n",
    "    color1 = \"#2E86AB\"\n",
    "    ax1.set_xlabel(\"Date\", fontsize=12, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Number of Sessions\", color=color1, fontsize=12, fontweight=\"bold\")\n",
    "    line1 = ax1.plot(\n",
    "        daily_stats[\"date\"],\n",
    "        daily_stats[\"sessions\"],\n",
    "        color=color1,\n",
    "        linewidth=2.5,\n",
    "        marker=\"o\",\n",
    "        markersize=6,\n",
    "        label=\"Sessions\",\n",
    "    )\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color1)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot events on right axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = \"#A23B72\"\n",
    "    ax2.set_ylabel(\"Number of Events\", color=color2, fontsize=12, fontweight=\"bold\")\n",
    "    line2 = ax2.plot(\n",
    "        daily_stats[\"date\"],\n",
    "        daily_stats[\"events\"],\n",
    "        color=color2,\n",
    "        linewidth=2.5,\n",
    "        marker=\"s\",\n",
    "        markersize=6,\n",
    "        linestyle=\"--\",\n",
    "        label=\"Events\",\n",
    "    )\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color2)\n",
    "\n",
    "    # Add phase markers (adjust dates according to your project)\n",
    "    # These are example dates - replace with actual phase transition dates\n",
    "    min_date = daily_stats[\"date\"].min()\n",
    "    max_date = daily_stats[\"date\"].max()\n",
    "    date_range = (max_date - min_date).days\n",
    "\n",
    "    # Example phase boundaries (adjust as needed)\n",
    "    alpha_end = min_date + timedelta(days=date_range * 0.2)\n",
    "    beta_closed_end = min_date + timedelta(days=date_range * 0.5)\n",
    "\n",
    "    # Add shaded regions for phases\n",
    "    ax1.axvspan(min_date, alpha_end, alpha=0.1, color=\"green\", label=\"Alpha Phase\")\n",
    "    ax1.axvspan(\n",
    "        alpha_end, beta_closed_end, alpha=0.1, color=\"orange\", label=\"Beta Closed\"\n",
    "    )\n",
    "    ax1.axvspan(beta_closed_end, max_date, alpha=0.1, color=\"blue\", label=\"Beta Open\")\n",
    "\n",
    "    # Title and legend\n",
    "    plt.title(\n",
    "        \"Session Activity Timeline Across Project Phases\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [str(line.get_label()) for line in lines]\n",
    "\n",
    "    ax1.legend(lines, labels, loc=\"upper left\", framealpha=0.9)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"SESSION ACTIVITY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total days: {len(daily_stats)}\")\n",
    "    print(f\"Avg sessions/day: {daily_stats['sessions'].mean():.1f}\")\n",
    "    print(\n",
    "        f\"Peak sessions: {daily_stats['sessions'].max()} on {daily_stats.loc[daily_stats['sessions'].idxmax(), 'date']}\"\n",
    "    )\n",
    "    print(f\"Avg events/day: {daily_stats['events'].mean():.1f}\")\n",
    "    print(\n",
    "        f\"Peak events: {daily_stats['events'].max()} on {daily_stats.loc[daily_stats['events'].idxmax(), 'date']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_session_activity_timeline(\n",
    "    events_df, save_path=\"viz1_session_activity_timeline.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: User Audience Distribution\n",
    "Shows breakdown of users by category and affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_user_audience_distribution(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a stacked bar chart showing user distribution by category and affiliation.\n",
    "\n",
    "    Uses real user profile information from sessionStart events.\n",
    "    \"\"\"\n",
    "    # Extract user profile data from sessionStart events\n",
    "    session_starts = events_df[events_df[\"eventType\"] == \"sessionStart\"].copy()\n",
    "\n",
    "    if len(session_starts) == 0:\n",
    "        print(\"No sessionStart events found with user profile data\")\n",
    "        return\n",
    "\n",
    "    # Extract profile information from augmented columns\n",
    "    user_profiles = []\n",
    "    for _, row in session_starts.iterrows():\n",
    "        # Get profile fields from augmented columns\n",
    "        organization = row.get(\"user_organization\", \"\")\n",
    "        usage = row.get(\"user_usage\", \"\")\n",
    "\n",
    "        # Map to category and affiliation\n",
    "        # Since the mock data doesn't have explicit category/affiliation,\n",
    "        # we'll use organization as affiliation and usage as category\n",
    "        category = usage if usage else \"Unknown\"\n",
    "        affiliation = organization if organization else \"Unknown\"\n",
    "\n",
    "        user_profiles.append(\n",
    "            {\n",
    "                \"category\": category,\n",
    "                \"affiliation\": affiliation,\n",
    "                \"sessionId\": row[\"sessionId\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not user_profiles:\n",
    "        print(\"No user profile data found in sessionStart events\")\n",
    "        print(\"Note: Profile fields may be empty in the dataset\")\n",
    "        return\n",
    "\n",
    "    profile_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "    # Filter out rows where both category and affiliation are \"Unknown\"\n",
    "    profile_df = profile_df[\n",
    "        ~(\n",
    "            (profile_df[\"category\"] == \"Unknown\")\n",
    "            & (profile_df[\"affiliation\"] == \"Unknown\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(profile_df) == 0:\n",
    "        print(\"No valid user profile data found (all fields are empty)\")\n",
    "        print(f\"Total sessions: {len(session_starts)}\")\n",
    "        return\n",
    "\n",
    "    # Create crosstab\n",
    "    crosstab = pd.crosstab(profile_df[\"category\"], profile_df[\"affiliation\"])\n",
    "\n",
    "    # Create stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    crosstab.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        colormap=\"tab20\",\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        \"User Audience Distribution by Category and Affiliation\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    plt.xlabel(\"User Category\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Number of Sessions\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.legend(title=\"Affiliation\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, label_type=\"center\", fmt=\"%.0f\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"USER AUDIENCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nBy Category:\")\n",
    "    print(profile_df[\"category\"].value_counts())\n",
    "    print(\"\\nBy Affiliation:\")\n",
    "    print(profile_df[\"affiliation\"].value_counts())\n",
    "\n",
    "\n",
    "plot_user_audience_distribution(\n",
    "    events_df, save_path=\"viz2_user_audience_distribution.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Query Type Classification\n",
    "Identifies main use cases through query categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_query_type_classification(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a bar chart showing query categories and subcategories.\n",
    "\n",
    "    Uses real query text from request events.\n",
    "    \"\"\"\n",
    "    # Filter for request events (which contain the actual queries)\n",
    "    queries = events_df[events_df[\"eventType\"] == \"request\"].copy()\n",
    "\n",
    "    if len(queries) == 0:\n",
    "        print(\"No request events found\")\n",
    "        return\n",
    "\n",
    "    # Extract query text from the augmented column\n",
    "    query_list = queries[\"query_text\"].dropna().tolist()\n",
    "\n",
    "    if not query_list:\n",
    "        print(\"No query text found in request events\")\n",
    "        return\n",
    "\n",
    "    # Categorize queries using real data\n",
    "    query_df = _categorize_queries(query_list)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    _plot_query_bar_chart(query_df, ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUERY TYPE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(query_df.to_string(index=False))\n",
    "    print(f\"\\nTotal queries: {query_df['count'].sum()}\")\n",
    "\n",
    "\n",
    "plot_query_type_classification(\n",
    "    events_df, save_path=\"viz3_query_type_classification.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: Session Depth Distribution\n",
    "Shows engagement levels through interaction counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_session_depth_distribution(events_df, save_path=None):\n",
    "    \"\"\"Create histogram showing distribution of interactions per session.\"\"\"\n",
    "    # Count events per session\n",
    "    session_stats_df = (\n",
    "        events_df.groupby(\"sessionId\").size().reset_index(name=\"event_count\")\n",
    "    )\n",
    "\n",
    "    # Cap at 21+ for better visualization\n",
    "    capped_counts = session_stats_df[\"event_count\"].clip(upper=21)\n",
    "\n",
    "    # Create histogram\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Left: Histogram with 21+ bin\n",
    "    ax1.hist(\n",
    "        capped_counts,\n",
    "        bins=range(1, 23),\n",
    "        align=\"left\",\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2,\n",
    "    )\n",
    "    ax1.set_title(\"Distribution of Events per Session\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.set_xlabel(\"Number of Events\", fontsize=11, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Number of Sessions\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    # Set x-ticks and relabel the last one as \"21+\"\n",
    "    xticks = list(range(1, 22))\n",
    "    labels = [str(x) if x < 21 else \"21+\" for x in xticks]\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Right: Box plot with statistics\n",
    "    ax2.boxplot(\n",
    "        session_stats_df[\"event_count\"],\n",
    "        vert=True,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightcoral\", alpha=0.7),\n",
    "        medianprops=dict(color=\"darkred\", linewidth=2),\n",
    "        whiskerprops=dict(color=\"black\", linewidth=1.5),\n",
    "        capprops=dict(color=\"black\", linewidth=1.5),\n",
    "    )\n",
    "\n",
    "    ax2.set_title(\"Session Depth Statistics\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Number of Events per Session\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_xticklabels([\"All Sessions\"])\n",
    "    ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add statistics as text\n",
    "    stats = session_stats_df[\"event_count\"].describe()\n",
    "    stats_text = f\"\"\"Mean: {stats[\"mean\"]:.1f}\n",
    "Median: {stats[\"50%\"]:.0f}\n",
    "Std: {stats[\"std\"]:.1f}\n",
    "Q1: {stats[\"25%\"]:.0f}\n",
    "Q3: {stats[\"75%\"]:.0f}\n",
    "Max: {stats[\"max\"]:.0f}\"\"\"\n",
    "\n",
    "    ax2.text(\n",
    "        1.15,\n",
    "        stats[\"75%\"],\n",
    "        stats_text,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"center\",\n",
    "    )\n",
    "\n",
    "    plt.suptitle(\"Session Engagement Analysis\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed statistics\n",
    "    print(\"SESSION DEPTH STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total sessions: {len(session_stats_df)}\")\n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(stats)\n",
    "    print(\"\\nEngagement categories:\")\n",
    "    print(\n",
    "        f\"  Low (1-5 events): {(session_stats_df['event_count'] <= 5).sum()} sessions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Medium (6-15 events): {((session_stats_df['event_count'] > 5) & (session_stats_df['event_count'] <= 15)).sum()} sessions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High (16+ events): {(session_stats_df['event_count'] > 15).sum()} sessions\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_session_depth_distribution(\n",
    "    events_df, save_path=\"viz4_session_depth_distribution.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: Response Quality Metrics Dashboard\n",
    "Quantifies system performance across key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_response_quality_dashboard(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a multi-panel dashboard showing key quality metrics.\n",
    "\n",
    "    Uses real data from events where available (feedback, response times, citations).\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Initialize metrics with defaults\n",
    "    metrics = {\n",
    "        \"Citation Accuracy\": 0.92,  # Mock - would need ground truth\n",
    "        \"User Satisfaction\": 0.78,  # Will be computed from real feedback\n",
    "        \"Response Completeness\": 0.85,  # Mock - would need evaluation\n",
    "        \"Hallucination Rate\": 0.08,  # Mock - would need evaluation\n",
    "        \"Source Coverage\": 0.88,  # Will be computed from citations\n",
    "        \"Response Time (avg)\": 2.3,  # Will be computed from real data\n",
    "    }\n",
    "\n",
    "    # Extract real feedback events\n",
    "    feedback_events = events_df[events_df[\"eventType\"].isin([\"thumbsUp\", \"thumbsDown\"])]\n",
    "    if len(feedback_events) > 0:\n",
    "        thumbs_up = (feedback_events[\"eventType\"] == \"thumbsUp\").sum()\n",
    "        thumbs_down = (feedback_events[\"eventType\"] == \"thumbsDown\").sum()\n",
    "        total_feedback = thumbs_up + thumbs_down\n",
    "        if total_feedback > 0:\n",
    "            metrics[\"User Satisfaction\"] = thumbs_up / total_feedback\n",
    "\n",
    "    # Compute real response time from request/response pairs\n",
    "    request_events = events_df[events_df[\"eventType\"] == \"request\"].copy()\n",
    "    response_events = events_df[events_df[\"eventType\"] == \"response\"].copy()\n",
    "\n",
    "    if len(request_events) > 0 and len(response_events) > 0:\n",
    "        # Match requests to responses by session and approximate timing\n",
    "        response_times = []\n",
    "        for _, req in request_events.iterrows():\n",
    "            # Find the next response in the same session\n",
    "            session_responses = response_events[\n",
    "                (response_events[\"sessionId\"] == req[\"sessionId\"])\n",
    "                & (response_events[\"timestamp\"] > req[\"timestamp\"])\n",
    "            ].sort_values(\"timestamp\")\n",
    "\n",
    "            if len(session_responses) > 0:\n",
    "                first_response = session_responses.iloc[0]\n",
    "                time_diff = (\n",
    "                    first_response[\"timestamp\"] - req[\"timestamp\"]\n",
    "                ).total_seconds()\n",
    "                if 0 < time_diff < 60:  # Reasonable response time (< 1 minute)\n",
    "                    response_times.append(time_diff)\n",
    "\n",
    "        if response_times:\n",
    "            metrics[\"Response Time (avg)\"] = sum(response_times) / len(response_times)\n",
    "\n",
    "    # Compute source coverage from citations\n",
    "    response_with_citations = events_df[\n",
    "        (events_df[\"eventType\"] == \"response\") & (events_df[\"citation_count\"] > 0)\n",
    "    ]\n",
    "    total_responses = len(events_df[events_df[\"eventType\"] == \"response\"])\n",
    "    if total_responses > 0:\n",
    "        metrics[\"Source Coverage\"] = len(response_with_citations) / total_responses\n",
    "\n",
    "    # Panel 1: Overall Score Gauge\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    overall_score = np.mean(\n",
    "        [\n",
    "            metrics[\"Citation Accuracy\"],\n",
    "            metrics[\"User Satisfaction\"],\n",
    "            metrics[\"Response Completeness\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create gauge\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    r = 1\n",
    "    ax1.plot(r * np.cos(theta), r * np.sin(theta), \"k-\", linewidth=3)\n",
    "    ax1.fill_between(\n",
    "        r * np.cos(theta), 0, r * np.sin(theta), alpha=0.2, color=\"lightblue\"\n",
    "    )\n",
    "\n",
    "    # Add colored segments\n",
    "    colors = [\"red\", \"orange\", \"yellow\", \"lightgreen\", \"green\"]\n",
    "    segments = 5\n",
    "    for i in range(segments):\n",
    "        theta_seg = np.linspace(i * np.pi / segments, (i + 1) * np.pi / segments, 20)\n",
    "        ax1.fill_between(\n",
    "            r * np.cos(theta_seg), 0, r * np.sin(theta_seg), alpha=0.3, color=colors[i]\n",
    "        )\n",
    "\n",
    "    # Add needle\n",
    "    needle_angle = overall_score * np.pi\n",
    "    ax1.plot(\n",
    "        [0, r * 0.9 * np.cos(needle_angle)],\n",
    "        [0, r * 0.9 * np.sin(needle_angle)],\n",
    "        \"r-\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    ax1.plot(0, 0, \"ro\", markersize=12)\n",
    "\n",
    "    # Labels\n",
    "    ax1.text(\n",
    "        0,\n",
    "        -0.3,\n",
    "        f\"Overall Quality Score\\n{overall_score:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax1.text(-1, 0, \"0%\", ha=\"right\", va=\"center\", fontsize=10)\n",
    "    ax1.text(1, 0, \"100%\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "    ax1.set_xlim(-1.3, 1.3)\n",
    "    ax1.set_ylim(-0.5, 1.3)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_aspect(\"equal\")\n",
    "\n",
    "    # Panel 2: Individual Metrics Bars\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    metric_names = [\n",
    "        \"Citation\\nAccuracy\",\n",
    "        \"User\\nSatisfaction\",\n",
    "        \"Response\\nCompleteness\",\n",
    "        \"Source\\nCoverage\",\n",
    "    ]\n",
    "    metric_values = [\n",
    "        metrics[\"Citation Accuracy\"],\n",
    "        metrics[\"User Satisfaction\"],\n",
    "        metrics[\"Response Completeness\"],\n",
    "        metrics[\"Source Coverage\"],\n",
    "    ]\n",
    "\n",
    "    colors_bars = [\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#06A77D\"]\n",
    "    bars = ax2.barh(metric_names, metric_values, color=colors_bars, edgecolor=\"black\")\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, metric_values)):\n",
    "        ax2.text(\n",
    "            val + 0.02, i, f\"{val:.1%}\", va=\"center\", fontweight=\"bold\", fontsize=11\n",
    "        )\n",
    "\n",
    "    ax2.set_xlim(0, 1.15)\n",
    "    ax2.set_xlabel(\"Score\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_title(\"Key Performance Metrics\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # Panel 3: Feedback Distribution\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    if len(feedback_events) > 0:\n",
    "        feedback_counts = feedback_events[\"eventType\"].value_counts()\n",
    "        colors_pie = [\"#06A77D\", \"#D62828\"]\n",
    "        ax3.pie(\n",
    "            feedback_counts.values,\n",
    "            labels=[\"üëç Positive\", \"üëé Negative\"],\n",
    "            autopct=\"%1.1f%%\",\n",
    "            colors=colors_pie,\n",
    "            startangle=90,\n",
    "            textprops={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    "        )\n",
    "        ax3.set_title(\n",
    "            f\"User Feedback\\n(n={len(feedback_events)})\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "    else:\n",
    "        ax3.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No feedback\\ndata available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        ax3.axis(\"off\")\n",
    "\n",
    "    # Panel 4: Hallucination Rate\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    hall_rate = metrics[\"Hallucination Rate\"]\n",
    "    colors_gauge = [\n",
    "        \"green\" if hall_rate < 0.1 else \"orange\" if hall_rate < 0.2 else \"red\"\n",
    "    ]\n",
    "    ax4.bar([\"Hallucination\\nRate\"], [hall_rate], color=colors_gauge, edgecolor=\"black\")\n",
    "    ax4.axhline(y=0.1, color=\"orange\", linestyle=\"--\", linewidth=2, label=\"Warning\")\n",
    "    ax4.axhline(y=0.2, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Critical\")\n",
    "    ax4.text(\n",
    "        0,\n",
    "        hall_rate + 0.02,\n",
    "        f\"{hall_rate:.1%}\",\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax4.set_ylim(0, 0.3)\n",
    "    ax4.set_ylabel(\"Rate\", fontsize=10, fontweight=\"bold\")\n",
    "    ax4.set_title(\"Hallucination Rate\", fontsize=11, fontweight=\"bold\")\n",
    "    ax4.legend(loc=\"upper right\", fontsize=8)\n",
    "    ax4.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Panel 5: Response Time\n",
    "    ax5 = fig.add_subplot(gs[2, 2])\n",
    "    resp_time = metrics[\"Response Time (avg)\"]\n",
    "    color_time = \"green\" if resp_time < 3 else \"orange\" if resp_time < 5 else \"red\"\n",
    "    ax5.bar([\"Avg Response\\nTime\"], [resp_time], color=color_time, edgecolor=\"black\")\n",
    "    ax5.text(\n",
    "        0,\n",
    "        resp_time + 0.2,\n",
    "        f\"{resp_time:.1f}s\",\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax5.set_ylabel(\"Seconds\", fontsize=10, fontweight=\"bold\")\n",
    "    ax5.set_title(\"Response Time\", fontsize=11, fontweight=\"bold\")\n",
    "    ax5.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Response Quality Metrics Dashboard\", fontsize=16, fontweight=\"bold\", y=0.98\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"QUALITY METRICS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for metric, value in metrics.items():\n",
    "        if \"Time\" in metric:\n",
    "            print(f\"{metric}: {value:.2f}s\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.1%}\")\n",
    "    print(\"\\nReal data used for:\")\n",
    "    print(f\"  - User Satisfaction: {len(feedback_events)} feedback events\")\n",
    "    print(f\"  - Source Coverage: {total_responses} responses analyzed\")\n",
    "    print(\n",
    "        f\"  - Response Time: {len(response_times) if 'response_times' in locals() else 0} request-response pairs\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_response_quality_dashboard(\n",
    "    events_df, save_path=\"viz5_quality_metrics_dashboard.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 6: Cost Analysis Over Time\n",
    "Tracks financial sustainability with component breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_analysis(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create stacked area chart showing costs over time by component.\n",
    "\n",
    "    Uses real activity data to estimate LLM costs based on actual usage.\n",
    "    \"\"\"\n",
    "    # Generate date range from actual data\n",
    "    dates = pd.date_range(\n",
    "        start=events_df[\"timestamp\"].min().date(),\n",
    "        end=events_df[\"timestamp\"].max().date(),\n",
    "        freq=\"D\",\n",
    "    )\n",
    "\n",
    "    # Count real API calls (request events) per day\n",
    "    daily_requests = (\n",
    "        events_df[events_df[\"eventType\"] == \"request\"]\n",
    "        .groupby(events_df[\"timestamp\"].dt.date)\n",
    "        .size()\n",
    "    )\n",
    "\n",
    "    cost_data = []\n",
    "    for date in dates:\n",
    "        # Real request count for this day\n",
    "        request_count = daily_requests.get(date.date(), 0)\n",
    "\n",
    "        # Estimate LLM cost based on real requests\n",
    "        # Assuming average cost per request (varies by model, tokens, etc.)\n",
    "        # Using a conservative estimate of $0.01 per request\n",
    "        llm_cost = request_count * 0.01\n",
    "\n",
    "        # Fixed costs (daily fraction of monthly costs)\n",
    "        hosting_cost = 6.67  # Daily fraction of monthly hosting (~$200/month)\n",
    "        storage_cost = 0.50  # Daily storage cost\n",
    "\n",
    "        cost_data.append(\n",
    "            {\n",
    "                \"date\": date,\n",
    "                \"LLM API\": llm_cost,\n",
    "                \"Hosting\": hosting_cost,\n",
    "                \"Storage\": storage_cost,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    cost_df = pd.DataFrame(cost_data)\n",
    "    cost_df[\"Total\"] = cost_df[[\"LLM API\", \"Hosting\", \"Storage\"]].sum(axis=1)\n",
    "    cost_df[\"Cumulative\"] = cost_df[\"Total\"].cumsum()\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Top: Stacked area chart of daily costs\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        0,\n",
    "        cost_df[\"LLM API\"],\n",
    "        label=\"LLM API\",\n",
    "        alpha=0.7,\n",
    "        color=\"#E63946\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"LLM API\"],\n",
    "        cost_df[\"LLM API\"] + cost_df[\"Hosting\"],\n",
    "        label=\"Hosting\",\n",
    "        alpha=0.7,\n",
    "        color=\"#F1FAEE\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"LLM API\"] + cost_df[\"Hosting\"],\n",
    "        cost_df[\"Total\"],\n",
    "        label=\"Storage\",\n",
    "        alpha=0.7,\n",
    "        color=\"#A8DADC\",\n",
    "    )\n",
    "\n",
    "    ax1.set_title(\"Daily Cost Breakdown by Component\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Cost (‚Ç¨)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax1.legend(loc=\"upper left\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    # Bottom: Cumulative cost line\n",
    "    ax2.plot(\n",
    "        cost_df[\"date\"],\n",
    "        cost_df[\"Cumulative\"],\n",
    "        linewidth=3,\n",
    "        color=\"#1D3557\",\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax2.fill_between(\n",
    "        cost_df[\"date\"], 0, cost_df[\"Cumulative\"], alpha=0.3, color=\"#457B9D\"\n",
    "    )\n",
    "\n",
    "    # Add milestone markers\n",
    "    total_cost = cost_df[\"Cumulative\"].iloc[-1]\n",
    "    ax2.axhline(y=total_cost, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "    ax2.text(\n",
    "        cost_df[\"date\"].iloc[-1],\n",
    "        total_cost + 10,\n",
    "        f\"Total: ‚Ç¨{total_cost:.2f}\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"yellow\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "    ax2.set_title(\"Cumulative Cost Over Time\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"Date\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Cumulative Cost (‚Ç¨)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Cost Analysis: Financial Sustainability Tracking\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.995,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print cost summary\n",
    "    print(\"COST ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Period: {cost_df['date'].min().date()} to {cost_df['date'].max().date()}\")\n",
    "    print(f\"\\nTotal API requests: {daily_requests.sum()}\")\n",
    "    print(\"\\nTotal costs:\")\n",
    "    print(f\"  LLM API: ‚Ç¨{cost_df['LLM API'].sum():.2f}\")\n",
    "    print(f\"  Hosting: ‚Ç¨{cost_df['Hosting'].sum():.2f}\")\n",
    "    print(f\"  Storage: ‚Ç¨{cost_df['Storage'].sum():.2f}\")\n",
    "    print(f\"  TOTAL: ‚Ç¨{cost_df['Total'].sum():.2f}\")\n",
    "    print(f\"\\nAverage daily cost: ‚Ç¨{cost_df['Total'].mean():.2f}\")\n",
    "    print(f\"Projected monthly cost: ‚Ç¨{cost_df['Total'].mean() * 30:.2f}\")\n",
    "    print(\n",
    "        \"\\nNote: LLM costs estimated at ‚Ç¨0.01 per request (actual costs vary by model/tokens)\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_cost_analysis(events_df, save_path=\"viz6_cost_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 8: Citation Network Visualization\n",
    "Reveals which research is most accessed and co-citation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_citation_network(events_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create network graph showing most-cited publications and co-citation patterns.\n",
    "\n",
    "    Uses real citation data from response events.\n",
    "    \"\"\"\n",
    "    # Extract citations from response events\n",
    "    response_events = events_df[events_df[\"eventType\"] == \"response\"].copy()\n",
    "\n",
    "    if len(response_events) == 0:\n",
    "        print(\"No response events found\")\n",
    "        return\n",
    "\n",
    "    # Collect all citations and their metadata\n",
    "    all_citations = []\n",
    "    citation_pairs = []  # For co-citation analysis\n",
    "\n",
    "    for _, row in response_events.iterrows():\n",
    "        citations = row.get(\"citations\", [])\n",
    "        if not isinstance(citations, list) or len(citations) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract document IDs from citations\n",
    "        doc_ids = []\n",
    "        for citation in citations:\n",
    "            if isinstance(citation, dict) and \"payload\" in citation:\n",
    "                payload = citation[\"payload\"]\n",
    "                if isinstance(payload, dict) and \"document_id\" in payload:\n",
    "                    doc_id = payload[\"document_id\"]\n",
    "                    doc_ids.append(doc_id)\n",
    "\n",
    "                    # Get document title for display\n",
    "                    metadata = payload.get(\"metadata\", {})\n",
    "                    title = metadata.get(\"title\", \"Unknown\")\n",
    "                    hal_id = metadata.get(\"hal_id\", \"unknown\")\n",
    "\n",
    "                    all_citations.append(\n",
    "                        {\"document_id\": doc_id, \"title\": title, \"hal_id\": hal_id}\n",
    "                    )\n",
    "\n",
    "        # Record co-citations (documents cited together in same response)\n",
    "        for i, doc1 in enumerate(doc_ids):\n",
    "            for doc2 in doc_ids[i + 1 :]:\n",
    "                citation_pairs.append((doc1, doc2))\n",
    "\n",
    "    if not all_citations:\n",
    "        print(\"No citation data found in response events\")\n",
    "        return\n",
    "\n",
    "    # Count citations per document\n",
    "    citation_df = pd.DataFrame(all_citations)\n",
    "    citation_counts = (\n",
    "        citation_df.groupby([\"document_id\", \"title\", \"hal_id\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    citation_counts = citation_counts.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    # Create simplified publication names for display\n",
    "    citation_counts[\"pub_name\"] = citation_counts.apply(\n",
    "        lambda row: f\"{row['hal_id'][:15]}...\"\n",
    "        if len(row[\"hal_id\"]) > 15\n",
    "        else row[\"hal_id\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Build citation counts dict for graph\n",
    "    citation_count_dict = dict(\n",
    "        zip(citation_counts[\"pub_name\"], citation_counts[\"count\"])\n",
    "    )\n",
    "    publications = citation_counts[\"pub_name\"].head(10).tolist()  # Top 10\n",
    "\n",
    "    # Build graph with real co-citation data\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with citation counts\n",
    "    for pub, count in citation_count_dict.items():\n",
    "        if pub in publications:\n",
    "            G.add_node(pub, citations=count)\n",
    "\n",
    "    # Add edges for co-citations\n",
    "    co_citation_counts = {}\n",
    "    for doc1, doc2 in citation_pairs:\n",
    "        # Find publication names for these document IDs\n",
    "        pub1 = citation_counts[citation_counts[\"document_id\"] == doc1][\n",
    "            \"pub_name\"\n",
    "        ].values\n",
    "        pub2 = citation_counts[citation_counts[\"document_id\"] == doc2][\n",
    "            \"pub_name\"\n",
    "        ].values\n",
    "\n",
    "        if len(pub1) > 0 and len(pub2) > 0:\n",
    "            pub1, pub2 = pub1[0], pub2[0]\n",
    "            if pub1 in publications and pub2 in publications:\n",
    "                pair = tuple(sorted([pub1, pub2]))\n",
    "                co_citation_counts[pair] = co_citation_counts.get(pair, 0) + 1\n",
    "\n",
    "    # Add edges with weights\n",
    "    for (pub1, pub2), weight in co_citation_counts.items():\n",
    "        G.add_edge(pub1, pub2, weight=weight)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "    # Left: Network graph\n",
    "    if G.number_of_nodes() > 0:\n",
    "        _draw_citation_network(G, ax1, publications)\n",
    "    else:\n",
    "        ax1.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient data\\nfor network visualization\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "    # Right: Bar chart of top cited publications\n",
    "    top_citations = citation_counts.head(10)\n",
    "    _draw_citation_bar_chart(\n",
    "        dict(zip(top_citations[\"pub_name\"], top_citations[\"count\"])), ax2\n",
    "    )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Citation Analysis: Research Impact and Connections\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print network statistics\n",
    "    print(\"CITATION NETWORK STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total unique documents cited: {len(citation_counts)}\")\n",
    "    print(f\"Total citation instances: {citation_counts['count'].sum()}\")\n",
    "    print(f\"Network nodes (top 10): {G.number_of_nodes()}\")\n",
    "    print(f\"Co-citation relationships: {G.number_of_edges()}\")\n",
    "    if G.number_of_nodes() > 0:\n",
    "        print(f\"Network density: {nx.density(G):.3f}\")\n",
    "    print(\"\\nTop 5 most cited documents:\")\n",
    "    for _, row in citation_counts.head(5).iterrows():\n",
    "        print(f\"  {row['hal_id']}: {row['count']} citations\")\n",
    "\n",
    "\n",
    "plot_citation_network(events_df, save_path=\"viz8_citation_network.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 11: User Journey Funnel\n",
    "Identifies friction points in user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Compute helpers ----------\n",
    "\n",
    "\n",
    "def _funnel_counts(events_df: pd.DataFrame) -> tuple[list[str], list[int]]:\n",
    "    stages = [\n",
    "        \"Landing\",\n",
    "        \"Session Start\",\n",
    "        \"First Query\",\n",
    "        \"Multiple\\nInteractions\\n(3+)\",\n",
    "        \"Feedback\\nProvided\",\n",
    "    ]\n",
    "    total_sessions = int(events_df[\"sessionId\"].nunique())\n",
    "    started = int(events_df.query(\"eventType == 'sessionStart'\")[\"sessionId\"].nunique())\n",
    "    queried = int(\n",
    "        events_df.query(\"eventType == 'messageSubmitted'\")[\"sessionId\"].nunique()\n",
    "    )\n",
    "\n",
    "    msg_counts = (\n",
    "        events_df.query(\"eventType == 'messageSubmitted'\").groupby(\"sessionId\").size()\n",
    "    )\n",
    "    engaged = int((msg_counts >= 3).sum())\n",
    "\n",
    "    feedback = int(\n",
    "        events_df.query(\"eventType in ['thumbsUp', 'thumbsDown']\")[\n",
    "            \"sessionId\"\n",
    "        ].nunique()\n",
    "    )\n",
    "\n",
    "    counts = [total_sessions, started, queried, engaged, feedback]\n",
    "    return stages, counts\n",
    "\n",
    "\n",
    "def _rates(counts: list[int]) -> tuple[list[float], list[float]]:\n",
    "    if not counts or counts[0] <= 0:\n",
    "        return [0.0] * len(counts), [0.0] * (len(counts) - 1)\n",
    "\n",
    "    base = counts[0]\n",
    "    conv = [100.0] + [c / base * 100.0 for c in counts[1:]]\n",
    "    drop = [\n",
    "        ((counts[i] - counts[i + 1]) / counts[i] * 100.0) if counts[i] > 0 else 0.0\n",
    "        for i in range(len(counts) - 1)\n",
    "    ]\n",
    "    return conv, drop\n",
    "\n",
    "\n",
    "# ---------- Plot helpers ----------\n",
    "\n",
    "\n",
    "def _plot_funnel(\n",
    "    ax: plt.Axes, stages: list[str], counts: list[int], conv: list[float]\n",
    ") -> None:\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(stages)))\n",
    "    max_width = max(counts) if counts else 1\n",
    "    scale = 10.0\n",
    "\n",
    "    for i, (stage, count, color) in enumerate(zip(stages, counts, colors)):\n",
    "        width = (count / max_width) * scale if max_width else 0.0\n",
    "        left = (scale - width) / 2.0\n",
    "\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (left, i), width, 0.8, facecolor=color, edgecolor=\"black\", linewidth=2\n",
    "            )\n",
    "        )\n",
    "        ax.text(\n",
    "            scale / 2.0,\n",
    "            i + 0.4,\n",
    "            stage,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.text(\n",
    "            scale + 1.0,\n",
    "            i + 0.4,\n",
    "            f\"{count}\\n({conv[i]:.1f}%)\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0, scale + 4)\n",
    "    ax.set_ylim(-0.5, len(stages))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"User Journey Funnel\", fontsize=13, fontweight=\"bold\", pad=20)\n",
    "\n",
    "\n",
    "def _plot_conversion(ax: plt.Axes, stages: list[str], conv: list[float]) -> None:\n",
    "    ax.plot(\n",
    "        stages, conv, linewidth=3, marker=\"o\", markersize=12, markerfacecolor=\"white\"\n",
    "    )\n",
    "    for i, rate in enumerate(conv):\n",
    "        ax.text(\n",
    "            i, rate + 3, f\"{rate:.1f}%\", ha=\"center\", fontweight=\"bold\", fontsize=10\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"Conversion Rate (%)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Conversion Rate Through Funnel Stages\", fontsize=13, fontweight=\"bold\"\n",
    "    )\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    # Quality bands\n",
    "    ax.axhspan(0, 30, alpha=0.1)\n",
    "    ax.axhspan(30, 60, alpha=0.1)\n",
    "    ax.axhspan(60, 100, alpha=0.1)\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Poor\"),\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Fair\"),\n",
    "            plt.Rectangle((0, 0), 1, 1, alpha=0.1, label=\"Good\"),\n",
    "        ],\n",
    "        loc=\"upper right\",\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- Reporting helper ----------\n",
    "\n",
    "\n",
    "def _print_funnel_report(\n",
    "    stages: list[str], counts: list[int], conv: list[float], drop: list[float]\n",
    ") -> None:\n",
    "    print(\"USER JOURNEY FUNNEL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nFunnel Statistics:\")\n",
    "    for i, (stage, count, rate) in enumerate(zip(stages, counts, conv), 1):\n",
    "        label = stage.replace(\"\\n\", \" \")\n",
    "        print(f\"\\n{i}. {label}:\\n   Count: {count}\\n   Conversion: {rate:.1f}%\")\n",
    "        if i - 1 < len(drop):\n",
    "            print(f\"   Drop-off to next stage: {drop[i - 1]:.1f}%\")\n",
    "\n",
    "    overall = conv[-1] if conv else 0.0\n",
    "    largest_drop = max(drop) if drop else 0.0\n",
    "    largest_drop_stage = (drop.index(largest_drop) + 1) if drop else 0\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"  Landing to Feedback: {overall:.1f}% conversion\")\n",
    "    print(f\"  Largest drop-off: {largest_drop:.1f}% at stage {largest_drop_stage}\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    if drop and drop[0] > 20:\n",
    "        print(\"  ‚ö† High drop-off after landing ‚Äî improve onboarding.\")\n",
    "    if len(drop) > 2 and drop[2] > 30:\n",
    "        print(\"  ‚ö† High drop-off after first query ‚Äî review response quality.\")\n",
    "    if overall < 20:\n",
    "        print(\"  ‚ö† Low feedback rate ‚Äî make feedback more prominent.\")\n",
    "\n",
    "\n",
    "# ---------- Orchestrator (low-complexity) ----------\n",
    "\n",
    "\n",
    "def plot_user_journey_funnel(\n",
    "    events_df: pd.DataFrame,\n",
    "    save_path: str | None = None,\n",
    "    *,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[plt.Figure, tuple[plt.Axes, plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Create funnel chart showing user progression through key stages.\n",
    "\n",
    "    Identifies drop-off points in the user journey.\n",
    "    \"\"\"\n",
    "    stages, counts = _funnel_counts(events_df)\n",
    "    conv, drop = _rates(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    _plot_funnel(ax1, stages, counts, conv)\n",
    "    _plot_conversion(ax2, stages, conv)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"User Journey Analysis: Conversion Funnel and Drop-off Points\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        _print_funnel_report(stages, counts, conv, drop)\n",
    "\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_journey_funnel(events_df, save_path=\"viz11_user_journey_funnel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 12: Topic Coverage Heat Map\n",
    "Identifies well-covered vs underserved research areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Data prep ----------\n",
    "\n",
    "\n",
    "def _default_themes() -> list[str]:\n",
    "    return [\n",
    "        \"Climate Policy\",\n",
    "        \"Energy Transition\",\n",
    "        \"Urban Planning\",\n",
    "        \"Transport\",\n",
    "        \"Circular Economy\",\n",
    "        \"Biodiversity\",\n",
    "        \"Water Management\",\n",
    "        \"Agriculture\",\n",
    "        \"Economic Modeling\",\n",
    "        \"Governance\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def _metric_labels() -> list[str]:\n",
    "    return [\n",
    "        \"Query\\nFrequency\",\n",
    "        \"Response\\nQuality\",\n",
    "        \"Citation\\nCoverage\",\n",
    "        \"User\\nSatisfaction\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def _make_mock_coverage(\n",
    "    themes: list[str], metrics: list[str], seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a mock coverage matrix scaled to [2, 10] with a few hand-tuned patterns.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = rng.random((len(themes), len(metrics))) * 8 + 2\n",
    "\n",
    "    theme_idx = {t: i for i, t in enumerate(themes)}\n",
    "    metric_idx = {m: j for j, m in enumerate(metrics)}\n",
    "\n",
    "    data[theme_idx[\"Climate Policy\"], metric_idx[\"Query\\nFrequency\"]] = 9.5\n",
    "    data[theme_idx[\"Energy Transition\"], metric_idx[\"Query\\nFrequency\"]] = 9.0\n",
    "    data[theme_idx[\"Water Management\"], metric_idx[\"Citation\\nCoverage\"]] = 8.5\n",
    "    data[theme_idx[\"Biodiversity\"], metric_idx[\"Citation\\nCoverage\"]] = 6.0\n",
    "\n",
    "    return pd.DataFrame(data, columns=_metric_labels(), index=themes)\n",
    "\n",
    "\n",
    "def _compute_summary(df: pd.DataFrame) -> tuple[pd.Series, pd.DataFrame]:\n",
    "    avg_by_theme = df.mean(axis=1)\n",
    "    ranked = df.copy()\n",
    "    ranked[\"Average\"] = avg_by_theme\n",
    "    return avg_by_theme, ranked\n",
    "\n",
    "\n",
    "# ---------- Plot helpers ----------\n",
    "\n",
    "\n",
    "def _plot_heatmap(ax: plt.Axes, df: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\"Heatmap with inline annotations and grid.\"\"\"\n",
    "    im = ax.imshow(df.values, cmap=\"RdYlGn\", aspect=\"auto\", vmin=2, vmax=10)\n",
    "\n",
    "    ax.set_xticks(np.arange(df.shape[1]))\n",
    "    ax.set_yticks(np.arange(df.shape[0]))\n",
    "    ax.set_xticklabels(list(df.columns), fontsize=11)\n",
    "    ax.set_yticklabels(list(df.index), fontsize=11)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            val = float(df.iat[i, j])\n",
    "            text_color = \"white\" if val < 6 else \"black\"\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{val:.1f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    ax.set_title(title, fontsize=13, fontweight=\"bold\", pad=15)\n",
    "    ax.set_xticks(np.arange(n_cols) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(n_rows) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle=\"-\", linewidth=1.5)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Score (2-10)\", rotation=270, labelpad=20, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "def _plot_radar(ax: plt.Axes, themes: list[str], avg_scores: list[float]) -> None:\n",
    "    \"\"\"Polar radar of average coverage by theme.\"\"\"\n",
    "    n = len(avg_scores)\n",
    "    angles = np.linspace(0, 2 * np.pi, n, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    scores = avg_scores + avg_scores[:1]\n",
    "\n",
    "    ax.plot(angles, scores, \"o-\", linewidth=3, markersize=8)\n",
    "    ax.fill(angles, scores, alpha=0.25)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_yticks([2, 4, 6, 8, 10])\n",
    "    ax.set_yticklabels([\"2\", \"4\", \"6\", \"8\", \"10\"], fontsize=9)\n",
    "\n",
    "    ax.plot(angles, [6] * len(angles), \"--\", linewidth=2, label=\"Target (6.0)\")\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(themes, fontsize=9)\n",
    "    ax.set_title(\n",
    "        \"Average Coverage Score by Research Theme\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.05))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# ---------- Orchestrator ----------\n",
    "\n",
    "\n",
    "def plot_topic_coverage_heatmap(\n",
    "    events_df: pd.DataFrame | None = None,\n",
    "    *,\n",
    "    themes: list[str] | None = None,\n",
    "    save_path: str | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[plt.Figure, tuple[plt.Axes, plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Create a matrix heatmap + radar showing CIRED research themes coverage.\n",
    "\n",
    "    Shows query frequency and response quality by research area.\n",
    "    \"\"\"\n",
    "    themes = themes or _default_themes()\n",
    "    metrics = _metric_labels()\n",
    "    coverage_df = _make_mock_coverage(themes, metrics)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax_left = plt.subplot(121)\n",
    "    ax_right = plt.subplot(122, projection=\"polar\")\n",
    "\n",
    "    _plot_heatmap(ax_left, coverage_df, \"Topic Coverage Heat Map by Metrics\")\n",
    "    avg_by_theme, ranked = _compute_summary(coverage_df)\n",
    "    _plot_radar(ax_right, themes, avg_by_theme.to_list())\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Research Topic Coverage Analysis: Identifying Gaps and Strengths\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        _print_coverage_analysis(coverage_df, ranked)\n",
    "\n",
    "    return fig, (ax_left, ax_right)\n",
    "\n",
    "\n",
    "# ---------- Reporting ----------\n",
    "\n",
    "\n",
    "def _print_coverage_analysis(coverage_df: pd.DataFrame, ranked: pd.DataFrame) -> None:\n",
    "    \"\"\"Console summary of coverage levels and weak spots.\"\"\"\n",
    "    print(\"TOPIC COVERAGE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for metric in _metric_labels():\n",
    "        avg = float(coverage_df[metric].mean())\n",
    "        label = metric.replace(\"\\n\", \" \")\n",
    "        print(f\"  {label}: {avg:.2f} (avg across themes)\")\n",
    "\n",
    "    sorted_themes = ranked.sort_values(\"Average\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 3 Well-Covered Themes:\")\n",
    "    for i, (theme, row) in enumerate(sorted_themes.head(3).iterrows(), 1):\n",
    "        print(f\"  {i}. {theme}: {row['Average']:.2f}\")\n",
    "\n",
    "    print(\"\\nBottom 3 Underserved Themes:\")\n",
    "    for i, (theme, row) in enumerate(sorted_themes.tail(3).iterrows(), 1):\n",
    "        print(f\"  {i}. {theme}: {row['Average']:.2f}\")\n",
    "\n",
    "    print(\"\\nRecommendations:\")\n",
    "    weak = ranked[ranked[\"Average\"] < 6]\n",
    "    if not weak.empty:\n",
    "        print(f\"  ‚ö† {len(weak)} theme(s) below target coverage (6.0):\")\n",
    "        for theme in weak.index:\n",
    "            print(\n",
    "                f\"    - {theme}: Consider adding more publications or improving indexing\"\n",
    "            )\n",
    "\n",
    "    for metric in _metric_labels():\n",
    "        low = coverage_df[coverage_df[metric] < 5]\n",
    "        if not low.empty:\n",
    "            label = metric.replace(\"\\n\", \" \")\n",
    "            print(f\"  ‚ö† Low {label} for: {', '.join(low.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_coverage_heatmap(events_df, save_path=\"viz12_topic_coverage_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Export\n",
    "\n",
    "Generate a summary report and export all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(events_df):\n",
    "    \"\"\"Generate a comprehensive text summary of all analyses.\"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"CIRED.DIGITAL PROJECT - FINAL ANALYSIS REPORT\")\n",
    "\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\n",
    "        f\"Analysis period: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\"\n",
    "    )\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"1. OVERALL STATISTICS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Total events: {len(events_df):,}\")\n",
    "    report.append(f\"Unique sessions: {events_df['sessionId'].nunique():,}\")\n",
    "    report.append(\n",
    "        f\"Date range: {(events_df['timestamp'].max() - events_df['timestamp'].min()).days} days\"\n",
    "    )\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"2. KEY FINDINGS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\n[Add key findings based on the 12 visualizations above]\")\n",
    "    report.append(\"- User engagement patterns: [...]\")\n",
    "    report.append(\"- Most popular research areas: [...]\")\n",
    "    report.append(\"- Quality metrics performance: [...]\")\n",
    "    report.append(\"- Cost sustainability: [...]\")\n",
    "    report.append(\"- Environmental impact: [...]\")\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"3. RECOMMENDATIONS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\\n[Add recommendations for:]\")\n",
    "    report.append(\"- System improvements\")\n",
    "    report.append(\"- User experience enhancements\")\n",
    "    report.append(\"- Cost optimization\")\n",
    "    report.append(\"- Content coverage expansion\")\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"END OF REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "\n",
    "    report_text = \"\\n\".join(report)\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"final_analysis_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(report_text)\n",
    "    print(\"\\n‚úì Report saved to: final_analysis_report.txt\")\n",
    "    print(\"‚úì All visualizations saved as PNG files (viz1_*.png through viz12_*.png)\")\n",
    "\n",
    "\n",
    "generate_summary_report(events_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
