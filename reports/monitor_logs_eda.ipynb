{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web App Event Logs - Exploratory Data Analysis\n",
    "\n",
    "This notebook analyzes web app event logs stored as JSON files in `reports/monitor-logs/YYYY/MM/DD/` structure.\n",
    "\n",
    "Each JSON file contains:\n",
    "```json\n",
    "{\n",
    "  \"sessionId\": \"session_1754052978056_4i3x2c3bc1g3g4j2\",\n",
    "  \"timestamp\": \"20250801T125618127Z\",\n",
    "  \"eventType\": \"sessionStart\",\n",
    "  \"payload\": { ... },\n",
    "  \"server_context\": { ... }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set up paths\n",
    "log_base_path = Path(\"monitor-logs\")\n",
    "print(f\"Base log path: {log_base_path}\")\n",
    "print(f\"Path exists: {log_base_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All JSON Files and Create DataFrame\n",
    "\n",
    "Find and load all JSON files, immediately converting to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_log_files(base_path=\"monitor-logs\", min_date=\"20250705\"):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the log directory structure.\n",
    "\n",
    "    Handles the recursive YYYY/MM/DD/ folder structure.\n",
    "    Each JSON file contains: sessionId, timestamp, eventType, payload, server_context.\n",
    "\n",
    "    Args:\n",
    "        base_path: Path to monitor-logs directory\n",
    "        min_date: Only load files from this date onwards (format: YYYYMMDD, e.g., '20250705')\n",
    "\n",
    "    \"\"\"\n",
    "    all_events = []\n",
    "    file_count = 0\n",
    "    error_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Use glob to find all JSON files recursively\n",
    "    json_pattern = os.path.join(base_path, \"**/*.json\")\n",
    "    json_files = glob.glob(json_pattern, recursive=True)\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files total\\n\")\n",
    "    if min_date:\n",
    "        print(f\"Filtering for files from {min_date} onwards (July 5, 2025)\\n\")\n",
    "\n",
    "    # Load each file\n",
    "    for file_path in json_files:\n",
    "        # Filter by date if min_date is specified\n",
    "        if min_date:\n",
    "            # Extract date from path: monitor-logs/YYYY/MM/DD/...\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            if len(path_parts) >= 4:\n",
    "                try:\n",
    "                    year = path_parts[-4]\n",
    "                    month = path_parts[-3]\n",
    "                    day = path_parts[-2]\n",
    "                    file_date = f\"{year}{month}{day}\"\n",
    "                    if file_date < min_date:\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                event_data = json.load(f)\n",
    "\n",
    "            # Validate required fields\n",
    "            if all(\n",
    "                key in event_data for key in [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "            ):\n",
    "                all_events.append(event_data)\n",
    "                file_count += 1\n",
    "            else:\n",
    "                error_count += 1\n",
    "\n",
    "        except (OSError, json.JSONDecodeError):\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Successfully loaded: {file_count} files\")\n",
    "    print(f\"Skipped (before cutoff): {skipped_count} files\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors encountered: {error_count} files\\n\")\n",
    "\n",
    "    return all_events\n",
    "\n",
    "\n",
    "# Load all events (from July 5, 2025 onwards)\n",
    "all_events = load_all_log_files(min_date=\"20250705\")\n",
    "\n",
    "print(f\"\\nTotal events loaded: {len(all_events)}\")\n",
    "\n",
    "# Create DataFrame immediately\n",
    "events_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Parse timestamp to datetime\n",
    "events_df[\"timestamp\"] = pd.to_datetime(\n",
    "    events_df[\"timestamp\"], format=\"%Y%m%dT%H%M%S%fZ\"\n",
    ")\n",
    "\n",
    "# Explode server_context into separate columns\n",
    "if \"server_context\" in events_df.columns:\n",
    "    server_context_df = pd.json_normalize(events_df[\"server_context\"].tolist())\n",
    "    # Prefix columns to avoid conflicts\n",
    "    server_context_df.columns = [\"server_\" + col for col in server_context_df.columns]\n",
    "    events_df = pd.concat([events_df, server_context_df], axis=1)\n",
    "    print(f\"\\nExpanded {len(server_context_df.columns)} server_context fields\")\n",
    "\n",
    "print(f\"\\nDataFrame created with shape: {events_df.shape}\")\n",
    "print(f\"Columns: {events_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics\n",
    "\n",
    "Get key statistics on number of events and unique sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total events:          {len(events_df):,}\")\n",
    "print(f\"Total unique sessions: {events_df['sessionId'].nunique():,}\")\n",
    "print(\n",
    "    f\"Avg events per session: {len(events_df) / events_df['sessionId'].nunique():.2f}\"\n",
    ")\n",
    "print(f\"\\nDate range: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\n",
    "\n",
    "# Show server context fields if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    print(\n",
    "        f\"\\nServer context fields: {', '.join([col.replace('server_', '') for col in server_cols])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Events by Type\n",
    "\n",
    "Breakdown of event types in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "event_type_df = events_df[\"eventType\"].value_counts().reset_index()\n",
    "event_type_df.columns = [\"eventType\", \"count\"]\n",
    "event_type_df[\"percentage\"] = (\n",
    "    event_type_df[\"count\"] / event_type_df[\"count\"].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "print(event_type_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Session Statistics\n",
    "\n",
    "Analyze events distribution across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SESSION EVENT DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count events per session\n",
    "session_stats_df = events_df.groupby(\"sessionId\").size().reset_index(name=\"event_count\")\n",
    "\n",
    "print(session_stats_df[\"event_count\"].describe())\n",
    "\n",
    "# Histogram of events per session (adding up every bin above 20 together)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Cap the data at 21 (anything >=21 becomes 21)\n",
    "capped_counts = session_stats_df[\"event_count\"].clip(upper=21)\n",
    "\n",
    "# Create histogram with bins from 1 to 22\n",
    "plt.hist(capped_counts, bins=range(1, 23), align=\"left\", color=\"skyblue\")\n",
    "\n",
    "plt.title(\"Distribution of Events per Session\")\n",
    "plt.xlabel(\"Number of Events\")\n",
    "plt.ylabel(\"Number of Sessions\")\n",
    "\n",
    "# Set x-ticks and relabel the last one as \"21+\"\n",
    "xticks = list(range(1, 22))\n",
    "labels = [str(x) if x < 21 else \"21+\" for x in xticks]\n",
    "plt.xticks(xticks, labels)\n",
    "\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Events\n",
    "\n",
    "Peek at sample events from each type to understand data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE EVENTS BY TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    sample = events_df[events_df[\"eventType\"] == event_type].iloc[0].to_dict()\n",
    "    print(f\"\\n{event_type.upper()}:\")\n",
    "    print(json.dumps(sample, indent=2, default=str)[:600])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataFrame Overview\n",
    "\n",
    "Display key information about the DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataFrame shape: {events_df.shape}\")\n",
    "print(f\"\\nColumns: {events_df.columns.tolist()}\")\n",
    "print(\"\\nData types:\")\n",
    "print(events_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display_cols = [\"sessionId\", \"timestamp\", \"eventType\"]\n",
    "# Add server context columns if available\n",
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "if server_cols:\n",
    "    display_cols.extend(server_cols[:3])  # Show first 3 server fields\n",
    "print(events_df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Server Context Analysis\n",
    "\n",
    "Analyze server-side context information (IP addresses, user agents, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_cols = [col for col in events_df.columns if col.startswith(\"server_\")]\n",
    "\n",
    "if server_cols:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SERVER CONTEXT FIELDS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for col in server_cols:\n",
    "        unique_count = events_df[col].nunique()\n",
    "        field_name = col.replace(\"server_\", \"\")\n",
    "        print(f\"\\n{field_name}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "\n",
    "        # Show top values for fields with reasonable cardinality\n",
    "        if unique_count <= 20:\n",
    "            print(f\"  Values: {events_df[col].value_counts().to_dict()}\")\n",
    "        elif unique_count < len(events_df) / 2:\n",
    "            print(\"  Top 5 values:\")\n",
    "            for val, count in events_df[col].value_counts().head(5).items():\n",
    "                print(f\"    {val}: {count}\")\n",
    "else:\n",
    "    print(\"No server_context fields found in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Payload Structure Analysis\n",
    "\n",
    "Analyze the structure of payload fields for each event type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PAYLOAD STRUCTURE BY EVENT TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract payload keys for each event type\n",
    "payload_structure = []\n",
    "for event_type in sorted(events_df[\"eventType\"].unique()):\n",
    "    type_df = events_df[events_df[\"eventType\"] == event_type]\n",
    "    all_keys = set()\n",
    "    for payload in type_df[\"payload\"]:\n",
    "        if isinstance(payload, dict):\n",
    "            all_keys.update(payload.keys())\n",
    "\n",
    "    payload_structure.append(\n",
    "        {\n",
    "            \"eventType\": event_type,\n",
    "            \"count\": len(type_df),\n",
    "            \"payload_keys\": sorted(list(all_keys)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "payload_df = pd.DataFrame(payload_structure)\n",
    "for _, row in payload_df.iterrows():\n",
    "    print(f\"\\n{row['eventType']} ({row['count']} events):\")\n",
    "    print(f\"  Keys: {', '.join(row['payload_keys'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Session Timeline Analysis\n",
    "\n",
    "Analyze event sequences and timing within sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENT COUNTS BY SESSION AND TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count events by session and type\n",
    "session_event_counts = (\n",
    "    events_df.groupby([\"sessionId\", \"eventType\"]).size().reset_index(name=\"count\")\n",
    ")\n",
    "session_event_counts = session_event_counts.sort_values(\n",
    "    [\"sessionId\", \"count\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "print(session_event_counts.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SESSION DURATION AND EVENT TIMING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate session duration and event rate\n",
    "session_timing = (\n",
    "    events_df.groupby(\"sessionId\")\n",
    "    .agg({\"timestamp\": [\"min\", \"max\", \"count\"]})\n",
    "    .reset_index()\n",
    ")\n",
    "session_timing.columns = [\"sessionId\", \"first_event\", \"last_event\", \"event_count\"]\n",
    "session_timing[\"duration_seconds\"] = (\n",
    "    session_timing[\"last_event\"] - session_timing[\"first_event\"]\n",
    ").dt.total_seconds()\n",
    "session_timing[\"events_per_minute\"] = (\n",
    "    session_timing[\"event_count\"] / (session_timing[\"duration_seconds\"] / 60)\n",
    ").round(2)\n",
    "\n",
    "print(session_timing.head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
